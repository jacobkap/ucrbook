[["index.html", "Uniform Crime Reporting (UCR) Program Data: An Opinionated Guide to FBI Data Chapter 1 Preface 1.1 Goal of the book 1.2 Structure of the book 1.3 Citing this book 1.4 Sources of UCR data 1.5 Recommended reading 1.6 How to contribute to this book 1.7 How to identify a particular agency (ORI codes) 1.8 The data as you get it from the FBI 1.9 Common issues", " Uniform Crime Reporting (UCR) Program Data: An Opinionated Guide to FBI Data Jacob Kaplan, Ph.D. 2024-09-13 Chapter 1 Preface If you have read an article about crime or arrests in the United States in the last half century, in most cases it was referring to the FBI’s Uniform Crime Reporting Program Data, otherwise known as UCR data. UCR data is, with the exception of the more detailed data that only covers murders, a monthly number of crimes or arrests reported to a single police agency which is then gathered by the FBI into one file that includes all reporting agencies. It is actually a collection of different datasets, all of which have information about crimes and arrests that occur in a particular jurisdiction. Think of your home town. This data will tell you how many crimes were reported for a small number of crime categories or how many people (broken down by age, sex, and race) were arrested for a (larger) set of crime categories in that city (if the city has multiple police agencies then each agency will report crimes/arrests under their jurisdiction though the largest agency - usually the local police department - will cover the vast majority of crimes/arrests in that city) in a given month. This is a very broad measure of crime, and its uses in research - or uses for understanding crime at all - is fairly limited. Yet is has become over much of the last century - and will likely remain among researchers for at least the next decade - the most important crime data in the United States. UCR data is important for three reasons: The definitions are standard, and all agencies (tend to) follow them so you can compare across agencies and over time.1 The data is available since 1960 (for most of the datasets) so there is a long period of available data.2 The data is available for most of the 18,000 police agencies in the United States so you can compare across agencies. More than many other datasets, there will be times when using UCR data that you will think “that is weird”. This book will cover this weirdness and when we think the weirdness is just an odd - but acceptable - quirk of the data, and when it is a sign of a big problem in the data or in that particular variable and that we should avoid using it. For most of this book we will be discussing the caveats of the above reasons - or, more directly, why these assumptions are wrong - but these are the reasons why the data is so influential. Figure 1.1: The annual percent of the United States population that is covered by an agency reporting data to NIBRS. Figure 1.2: The percent of each state’s population that is covered by police agencies reporting at least one month of data to NIBRS, 2022. 1.1 Goal of the book By the end of each chapter you should have a firm grasp on the dataset that is covered and how to use it properly. However, this book cannot possibly cover every potential use case for the data so make sure to carefully examine the data yourself for your own particular use. I get a lot of emails from people asking questions about this data so my own goal is to create a single place that answers as many questions as I can about the data. Again, this is among the most commonly used crime datasets and there are still many current papers published with incorrect information about the data (including such simple aspects like what geographic unit data is in and what time unit it is in). So hopefully this book will decrease the number of misconceptions about this data, increasing overall research quality. 1.2 Structure of the book This book will be divided into ten chapters: this chapter, an intro chapter briefly summarizing each dataset and going over overall issues with UCR data, and seven chapters each covering one of the seven UCR datasets. The final chapter will cover county-level UCR data, a commonly used but highly flawed aggregation of UCR data that I recommend against using. Each chapter will follow the same format: we will start with a brief summary of the data such as when it first because available and how it can be used. Next we will look at how many agencies report their data to this dataset, often looking at how to measure this reporting rate a couple of different ways. Finally, we will cover the important variables included in the data and how to use them properly (including not using them at all) - this will be the bulk of each chapter. 1.3 Citing this book If this data was useful in your research, please cite it. To cite this book, please use the below citation: Kaplan J (2024). Uniform Crime Reporting (UCR) Program Data: An Opinionated Guide to FBI Data. https://ucrbook.com/. BibTeX format: @Manual{ucrbook, title = {Uniform Crime Reporting (UCR) Program Data: An Opinionated Guide to FBI Data}, author = {{Jacob Kaplan}}, year = {2024}, url = {https://ucrbook.com/}, } 1.4 Sources of UCR data 1.4.1 My own collection 1.4.1.1 openICPSR 1.4.1.2 [Crimedatatool.com][https://crimedatatool.com/] 1.4.2 NACJD 1.4.3 FBI (raw data) 1.4.4 Raw data 1.4.5 Crime Data Explorer 1.4.6 Crimes in the United States report 1.4.7 FBI (Crime Data Explorer) 1.4.8 FBI (Crimes in the United States Report) There are a few different sources of UCR data available today. First, and probably most commonly used, is the data put together by the National Archive of Criminal Justice Data (NACJD)). This a team out of the University of Michigan who manages a huge number of criminal justice datasets and makes them available to the public. If you have any questions about crime data - UCR or other crime data - I highly recommend you reach out to them for answers. They have a collection of data and excellent documentation available for UCR data available on their site here. One limitation to their data, however, is that each year of data is available as an individual file meaning that you will need to concatenate each year together into a single file. Some years also have different column names (generally minor changes like spelling robbery “rob” one year and “robb” the next) which requires more work to standardize before you could concatenate. They also only have data through 2016 which means that the most recent years (UCR data is available through 2019) of data are (as of this writing) unavailable. Next, and most usable for the general public - but limited for researchers - is the FBI’s official website Crime Data Explorer. On this site you can chose an agency and see annual crime data (remember, UCR data is monthly so this is not as detailed as it can be) for certain crimes (and not even all the crimes actually available in the data). This is okay for the general public but only provides a fraction of the data available in the actual data so is really not good for researchers. It is worth mentioning a final source of UCR information. This is the annual Crimes in the United States report released by the FBI each year around the start of October. As an example, here is the website for the 2019 report. In this report is summarized data which in most cases estimates missing data and provides information about national and subnational (though rarely city-level) crime data. As with the FBI’s site, it is only a fraction of the true data available so is not a very useful source of crime data for quality research. Still, this is a very common source of information used by researchers. 1.5 Recommended reading While this book is designed to help researchers use this data, the FBI has an excellent manual on this data designed to help police agencies submit their data. That manual, called the “Summary Reporting System (SRS) User Manual” provides excellent definitions and examples of many variables included in the data. In this book when I quote the FBI, such as defining a crime, I quote from this manual. The manual is available to download as a PDF on the FBI’s site and I have also posted it on my GitHub page here for convenience. I highly recommend that you read this manual before using the data. That manual, alongside this book which tries to explain when and how the agencies do not follow the manual, will provide a solid foundation for your understanding of UCR data. 1.6 How to contribute to this book If you have any questions, suggestions (such as a topic to cover), or find any issues, please make a post on the Issues page for this book on GitHub. On this page you can create a new issue (which is basically just a post on this forum) with a title and a longer description of your issue. You will need a GitHub account to make a post. Posting here lets me track issues and respond to your message or alert you when the issue is closed (i.e. I have finished or denied the request). Issues are also public so you can see if someone has already posted something similar. For more minor issues like typos or grammar mistakes, you can edit the book directly through its GitHub page. That will make an update for me to accept, which will change the book to include your edit. To do that, click the edit button at the top of the site - the button is highlighted in the below figure. You will need to make a GitHub account to make edits. When you click on that button you will be taken to a page that looks like a Word Doc where you can make edits. Make any edits you want and then scroll to the bottom of the page. There you can write a short (please, no more than a sentence or two) description of what you have done and then submit the changes for me to review. Figure 1.3: The edit button for how to make edits of this book. Please only use the above two methods to contribute or make suggestions about the book. While it is a bit more work for you to do it this way, since you will need to make a GitHub account if you do not already have one, it helps me organize all the questions in one place and update the book if I decide to add answers to certain questions. 1.7 How to identify a particular agency (ORI codes) In NIBRS and other FBI data sets, agencies are identified using ORiginating Agency Identifiers or an ORI. An ORI is a unique ID code used to identify an agency.3 If we used the agency’s name we would end up with some duplicates since there can be multiple agencies in the country (and in a state, those this is very rare) with the same name. For example, if you looked for the Philadelphia Police Department using the agency name, you would find both the “Philadelphia Police Department” in Pennsylvania and the one in Mississippi. Each ORI is a 9-digit value starting with the state abbreviation4 followed by 7 numbers. In the UCR data (another FBI data set) the ORI uses only a 7-digit code - with only the 5 numbers following the state abbreviation instead of 7. So the NIBRS ORI codes are sometimes called ORI9. For nearly all agencies, the only difference between the UCR ORI and the NIBRS ORI is that the NIBRS ORI has “00” at the end so it is technically 9 characters long but is not any more specific than the 7-character UCR ORI code. When dealing with specific agencies, make sure to use the ORI rather than the agency name to avoid any mistakes. For an easy way to find the ORI number of an agency, use this page on my site. Type an agency name or an ORI code into the search section and it will return everything that is a match. 1.8 The data as you get it from the FBI We will finish this overview of the SRS data by briefly talking about format of the data that is released by the FBI, before the processing done by myself or NACJD that converts the data to a type that software like R or Stata or Excel can understand. The FBI releases their data as fixed-width ASCII files which are basically just an Excel file but with all of the columns squished together. As an example, Figure 1.4 shows what the data looks like as you receive it from the FBI for the Offenses Known and Clearances by Arrest dataset for 1960, the first year with data available. In the figure, it seems like there are multiple rows but that is just because the software that I opened the file in is not wide enough - in reality what is shown is a single row that is extremely wide because there are over 1,500 columns in this data. If you scroll down enough you will see the next row, but that is not shown in the current image. What is shown is a single row with a ton of columns all pushed up next to each other. Since all of the columns are squished together (the gaps are just blank spaces because the value there is a space, but that does not mean there is a in the data. Spaces are possible values in the data and are meaningful), you need some way to figure out which parts of the data belong in which column. Figure 1.4: Fixed-width ASCII file for the 1960 Offenses Known and Clearances by Arrest dataset. Figure 1.5: Fixed-width ASCII file for the 1991 National Incident-Based Reporting System (NIBRS) dataset. The “fixed-width” part of the file type is how this works (the ASCII part basically means it is a text file). Each row is the same width - literally the same number of characters, including blank spaces. So you must tell the software you are using to process this file - by literally writing code in something called a “setup file” but is basically just instructions for whatever software you use (R, SPSS, Stata, SAS can all do this) - which characters are certain columns. For example, in this data the first character says which type of SRS data it is (1 means the Offenses Known and Clearances by Arrest data) and the next two characters (in the setup file written as 2-3 since it is characters 2 through 3 [inclusive]) are the state number (01 is the state code for Alabama). So we can read this row as the first column indicating it is an Offenses Known data, the second column indicating that it is for the state of Alabama, and so on for each of the remaining columns. To read in this data you will need a setup file that covers every column in the data (some software, like R, can handle just reading in the specific columns you want and do not need to include every column in the setup file). The second important thing to know about reading in a fixed-width ASCII file is something called a “value label.”5 For example, in the above image we saw the characters 2-3 is the state and in the row we have the value “01” which means that the state is “Alabama.” Since this type of data is trying to be as small as efficient as possible, it often replaces longer values with shorter one and provides a translation for the software to use to convert it to the proper value when reading it. “Alabama” is more characters than “01” so it saves space to say “01” and just replace that with “Alabama” later on. So “01” would be the “value” and “Alabama” would be the “label” that it changes to once read. Fixed-width ASCII files may seem awful to you reading it today, and it is awful to use. But it appears to be an efficient way to store data back many decades ago when data releases began but now is extremely inefficient - in terms of speed, file size, ease of use - compared to modern software so I am not sure why they still release data in this format. But they do, and even the more modern NIBRS data comes in this format. For you, however, the important part to understand is not how exactly to read this type of data, but to understand that people who made this data publicly available (such as myself and the team at NACJD) must make this conversion process.6 This conversion process, from fixed-width ASCII to a useful format is the most dangerous step taken in using this data - and one that is nearly entirely unseen by researchers. Every line of code you write (or, for SPSS users, click you make) invites the possibility of making a mistake.7 The FBI does not provide a setup file with the fixed-width ASCII data so to read in this data you need to make it yourself. Since some SRS data are massive, this involves assigning the column width for thousands of columns and the value labels for hundreds of different value labels.8 A typo anywhere could have potentially far-reaching consequences, so this is a crucial weak point in the data cleaning process - and one in which I have not seen anything written about before. While I have been diligent in checking the setup files and my code to seek out any issues - and I know that NACJD has a robust checking process for their own work - that does not mean our work is perfect.9 Even with perfection in processing the raw data to useful files, decisions we make (e.g. what level to aggregate to, what is an outlier) can affect both what type of questions you can ask when using this data, and how well you can answer them. 1.9 Common issues In this section we will discuss issues common to most or all of the SRS datasets. For some of these, we will come back to the issues in more detail in the chapter for the datasets most affected by the problem. 1.9.1 Population Each of the SRS datasets include a population variable that has the estimated population under the jurisdiction of that agency.10 This variable is often used to create crime rates that control for population. In cases where jurisdiction overlaps, such as when a city has university police agencies or county sheriffs in counties where the cities in that county have their own police, SRS data assigns the population covered to the most local agency and zero population to the overlapping agency. So an agency’s population is the number of people in that jurisdiction that is not already covered by a different agency. For example, the city of Los Angeles in California has nearly four million residents according to the US Census. There are multiple police agencies in the city, including the Los Angeles Police Department, the Los Angeles County Sheriff’s Office, the California Highway Patrol that operates in the area, airport and port police, and university police departments. If each agency reported the number of people in their jurisdiction - which all overlap with each other - we would end up with a population far higher than LA’s four million people. To prevent double-counting population when agency’s jurisdictions overlap, the non-primary agency will report a population of 0, even though they still report crime data like normal. As an example, in 2018 the police department for California State University - Los Angeles reported 92 thefts and a population of 0. Those 92 thefts are not counted in the Los Angeles Police Department data, even though the department counts the population. To get complete crime counts in Los Angeles, you would need to add up all police agencies within in the city; since the population value is 0 for non-LAPD agencies, both the population and the crime sum will be correct. The SRS uses this method even when only parts of a jurisdiction overlaps. Los Angeles County Sheriff has a population of about one million people, far less than the actual county population (the number of residents, according to the Census) of about 10 million people. This is because the other nine million people are accounted for by other agencies, mainly the local police agencies in the cities that make up Los Angeles County. The population value is the population who reside in that jurisdiction and does not count people who are in the area but do not live there, such as tourists or people who commute there for work. This means that using the population value to determine a rate can be misleading as some places have much higher numbers of non-residents in the area (e.g. Las Vegas, Washington D.C.) than others. 1.9.2 Voluntary reporting When an agency reports their data to the FBI, they do so voluntarily - there is no national requirement to report.11 This means that there is inconsistency in which agencies report, how many months of the year they report for, and which variables they include in their data submissions. In general, more agencies report their data every year and once an agency begins reporting data they tend to keep reporting. The SRS datasets are a collection of separate, though related, datasets and an agency can report to as many of these datasets as they want - an agency that reports to one dataset does not mean that they report to other datasets. Figure 1.6 shows the number of agencies that submitted at least one month of data to the Offenses Known and Clearances by Arrest data in the given year. For the first decade of available data under 8,000 agencies reported data and this grew to over 13,500 by the late 1970s before plateauing for about a decade. The number of agencies that reported their data actually declined in the 1990s, driven primarily by many Florida agencies temporarily dropping out, before growing steadily to nearly 17,000 agencies in 2010; from here it kept increasing but slower than before. Figure 1.6: The annual number of agencies reporting to the Offenses Known and Clearances by Arrest dataset. Reporting is based on the agency reporting at least one month of data in that year. There are approximately 18,000 police agencies in the United States so recent data has reports from nearly all agencies, while older data has far fewer agencies reporting. When trying to estimate to larger geographies, such as state or national-level, later years will be more accurate as you are missing less data. For earlier data, however, you are dealing with a smaller share of agencies meaning that you have a large amount of missing data and a less representative sample. Figure 1.7 repeats the above figure but now including only agencies with 100,000 people or more in their jurisdiction. While these agencies have a far more linear trend than all agencies, the basic lesson is the same: recent data has most agencies reporting; old data excludes many agencies. Figure 1.7: The annual number of agencies with a population of 100,000 or higher reporting to the Offenses Known and Clearances by Arrest dataset. Reporting is based on the agency reporting at least one month of data in that year. This voluntariness extends beyond whether they report or not, but into which variables they report. While in practice most agencies report every crime when they report any, they do have the choice to report only a subset of offenses. This is especially true for subsets of larger categories - such as gun assaults, a subset of aggravated assaults, or marijuana possession arrests which is a subset of drug possession arrests. As an example, Figure 1.8 shows the annual number of aggravated assaults with a gun in New York City. In 2003 the New York Police Department stopped reporting this category of offense, resuming only in 2013. They continued to report the broader aggravated assault category, but not any of the subsections of aggravated assaults which say which weapon was used during the assault. Figure 1.8: Monthly reports of gun assaults in New York City, 1960-2022. Given that agencies can join or drop out of the SRS program at will, and report only partial data, it is highly important to carefully examine your data to make sure that there are no issues caused by this. Even when an agency reports SRS data, and even when they report every crime category, they can report fewer than 12 months of data. In some cases they simply report all of their data in December, or report quarterly or semi-annually so some months have zero crimes reported while others count multiple months in that month’s data. One example of this is New York City, shown in Figure 1.9, in the early-2000s to the mid-2010s where they began reporting data quarterly instead of monthly. Figure 1.9: Monthly murders in New York City, 1990-2022. During the 2000s, the police department began reporting quarterly instead of monthly and then resumed monthly reporting. When you sum up each month into an annual count, as shown in Figure 1.10, the problem disappears since the zero months are accounted for in the months that have the quarterly data. If you are using monthly data and only examine the data at the annual level, you will fall into the trap of having incorrect data that is hidden due to the level of aggregation examined. While cases like NYC are obvious when viewed monthly, for people that are including thousands of agencies in their data, it is unfeasible to look at each agency for each crime included. This can introduce errors as the best way to examine the data is manually viewing graphs and the automated method, looking for outliers through some kind of comparison to expected values, can be incorrect. Figure 1.10: Annual murders in New York City, 1990-2022. In other cases when agencies report fewer than 12 months of the year, they simply report partial data and as a result undercount crimes. Figure 1.11 shows annual murders in Miami-Dade, Florida and has three years of this issue occurring. The first two years with this issue are the two where zero murders are reported - this is because the agency did not report any months of data. The final year is in 2018, the last year of data in this graph, where it looks like murder suddenly dropped significantly. That is just because Miami-Dade only reported through June, so they are missing half of 2018. Figure 1.11: Annual murders in Miami-Dade, Florida, 1960-2022. 1.9.3 Zero crimes vs no reports When an agency does not report, we see it in the data as reporting zero crimes, not reporting NA or any indicator that they did not report. In cases where the agency says they did not report that month we can be fairly sure (not entirely since that variable is not always accurate) that the zero crimes reported are simply that the agency did not report. In cases where the agency says they report that month but report zero crimes, we cannot be sure if that is a true no crimes reported to the agency or the agency not reporting to the SRS. As agencies can report some crimes but not others in a given month and still be considered reporting that month, just saying they reported does not mean that the zero is a true zero. In some cases it is easy to see when a zero crimes reported is actually the agency not reporting. As Figure 1.8 shows with New York City gun assaults, there is a massive and sustained drop-off to zero crimes and then a sudden return years later. Obviously, going from hundreds of crimes to zero crimes is not a matter of crimes not occurring anymore, it is a matter of the agency not reporting - and New York City did report other crimes these years so in the data it says that they reported every month. So in agencies which tend to report many crimes - and many here can be a few as 10 a year since going from 10 to 0 is a big drop - a sudden report of zero crimes is probably just non-reporting. Differentiating zero crimes and no reports becomes tricky in agencies that tend to report few crimes, which most small towns do. As an example, Figure 1.12 shows the annual reports of rape in Danville, California, a city of approximately 45,000 people. The city reports on average 2.8 rapes per year and in five years reported zero rapes. In cases like this it is not clear whether we should consider those zero years as true zeros (that no one was raped or reported their rape to the police) or whether the agency simply did not report rape data that year. Figure 1.12: Annual rapes reported in Danville, CA, 1960-2022. 1.9.4 Agency data covered by another agency We will see many examples of when agencies do not follow the definitions, which really limits this data.↩︎ While the original UCR data first reported in 1929, there is only machine-readable data since 1960.↩︎ This is referred to as an “ORI”, “ORI code”, and “ORI number”, all of which mean the same thing.↩︎ The abbreviation for Nebraska is “NB” rather than the more commonly used “NE.”↩︎ For most fixed-width ASCII files there are also missing values where it will have placeholder value such as -8 and the setup file will instruct the software to convert that to NA. SRS data, however, does not have this and does not indicate when values are missing in this manner.↩︎ For those interested in reading in this type of data, please see my R package asciiSetupReader.↩︎ Even highly experienced programmers who are doing something like can make mistakes. For example, if you type out “2+2” 100 times - something extremely simple that anyone can do - how often will you mistype a character and get a wrong result? I would guess that at least once you would make a mistake.↩︎ With the exception of the arrest data and some value label changes in hate crimes and homicide data, the setup files remain consistent so a single file will work for all years for a given dataset. You do not need to make a setup file for each year.↩︎ For evidence of this, please see any of the openICPSR pages for my detail as they detail changes I have made in the data such as decisions on what level to aggregate to and mistakes that I made and later found and fixed.↩︎ Jurisdiction here refers to the boundaries of the local government, not any legal authority for where the officer can make arrests. For example, the Los Angeles Police Department’s jurisdiction in this case refers to crimes that happen inside the city or are otherwise investigated by the LAPD - and are not primarily investigated by another agency.↩︎ Some states do mandate that their agencies report, but this is not always followed.↩︎ "],["about-the-author.html", "About the Author", " About the Author Jacob Kaplan I am a Professional Specialist at the School of Public and International Affairs (SPIA) and a member of Criminal Justice @ SPIA at Princeton University. My research focuses on law enforcement, including its impact on violent crime, the influence of removing ‘bad apple’ officers on reducing complaints against officers, the extent to which police forces represent the civilian populations they serve, and the role of race and political affiliations in shaping officer behavior. In addition to this, I conduct methodological research focused on the quality and usability of crime data, with a special emphasis on the FBI’s Uniform Crime Reporting (UCR) Program. I am the author of A Criminologist’s Guide to R: Crime by the Numbers (Chapman &amp; Hall/CRC The R Series, 2022), an introductory textbook on the R programming language tailored for crime research, with a special focus on data cleaning and analysis. In addition, I have developed several R packages, including fastDummies, asciiSetupReader, and predictrace, to streamline the data analysis process for researchers. My website, Crime Data Tool, offers users an interactive platform to explore crime data from thousands of agencies across hundreds of variables (e.g., arrests, offenses, demographics)—no data or programming skills required. My research has been published in leading academic journals, such as the Journal of Quantitative Criminology, Journal of Research in Crime and Delinquency, Journal of Interpersonal Violence, and American Political Science Review. I hold a B.S. in Criminal Justice from California State University, Sacramento, and a M.S. and Ph.D. in Criminology from the University of Pennsylvania. I previously served on the FBI’s Criminal Justice Information Services (CJIS) Advisory Policy Board (APB) Uniform Crime Reporting (UCR) Subcommittee. "],["shr.html", "Chapter 2 Supplementary Homicide Reports (SHR) 2.1 Agencies reporting 2.2 Important variables", " Chapter 2 Supplementary Homicide Reports (SHR) The Supplementary Homicide Reports dataset - often abbreviated to SHR - is the most detailed of the SRS datasets and provides information about the circumstances and participants (victim and offender demographics and relationship status) for homicides. For each homicide incident it tells you the age, gender, race, and ethnicity of each victim and offender as well as the relationship between the first victim and each of the offenders (but not the other victims in cases where there are multiple victims). It also tells you the weapon used by each offender and the circumstance of the killing, such as a “lovers triangle” or a gang-related murder. As with other SRS data, it also tells you the agency it occurred in and the month and year when the crime happened. One important point of clarification: this is not the number of murders, though it does track that. This data also includes the number of homicides that are manslaughter by negligence (e.g. children playing with a gun, hunting accident) and justifiable homicides (i.e. not criminal). So be carefully when speaking about this data. It is murders but not only murders so you want to speak precisely. 2.1 Agencies reporting This data only has a report when the agency has a homicide that year and since homicides are relatively rare it is difficult to measure underreporting. One way we can look at reporting is to compare homicide in the SHR data with that of other datasets. We will look at two of them: the Offenses Known and Clearances by Arrest which is covered in detail in Chapter ??, and the Center for Disease Control and Prevention (CDC) data on national deaths from homicide.12 Both this dataset and the Offenses Known and Clearances by Arrest data are SHR datasets so you may think that the numbers of homicides from each dataset should be the same. That is a perfectly reasonable assumption, but since this is SHR data we are talking about, you would be wrong. Police agencies are free to report to either, both, or neither dataset so while the number of homicides are close for each dataset, they are never equal. CDC WONDER data aggregates mortality data (among other data) from state death certificates which reduces the issue of voluntary reporting that we have in SHR data. Figure 2.1 shows the annual number of homicide victims (including murders and manslaughters) from each of these datasets starting in 1976 for the SHR data and in 1999 for the CDC data.13 For the SHR data, in every year the numbers are fairly similar and the trends are the same over time, but the number of homicides is never equal. The numbers have actually gotten worse over time with the difference between the datasets increasing and the Offenses Known data having consistently more murders reported than the SHR data since the late 1990s. Compared to the CDC data, however, both SHR datasets - and in particular the SHR data - undercount the number of homicides. While trends are the same, SHR data reports thousands fewer murders per year than the CDC data, indicating how much of an issue underreporting is in this data. Figure 2.1: The annual number of murders and nonngeligent manslaughters from the Supplementary Homicide Report and the Offenses Known and Clearances by Arrest dataset, and homicides from the Center for Disease Control (CDC). Numbers differ because agencies voluntarily report and may not report to both datasets. Let us look at Chicago for another example of the differences in reporting from the SHR and the Offenses Known data. Figure 2.2 shows the annual number of homicide victims from both datasets. In most years they are pretty similar, excluding a few really odd years in the 1980s and in 1990. But what is also strange is that most years have more SHR victims than Offenses Known victims. So nationally SHR has fewer homicides than Offenses Known but that pattern is reversed in Chicago? This is one of the many quirks of SHR data. And is a warning against treating national trends as local trends; what is true nationally is not always true in your community. So when you use this data, check everything closely. And once you have done that, check it again. Figure 2.2: The annual number of homicide victims in Chicago, SHR and Offeksnes Known, 1976-2022. Another way to visualize reporting is to see the total number of agencies that report at least one homicide, as depicted in Figure 2.3. Here we can see that have about 3,000 agencies reporting. Given that most agencies are small and truly do have zero homicides in a year, that may be reasonable. Agencies that do not have homicides do not submit a report saying so, they just do not submit any data. So that makes it hard to tell when an agency not reporting data is doing so because they choose to not report, or because they have nothing to report. This is most common in small agencies where many years truly have no homicides. But let us look at our biggest agencies, and see how much of an impact it would make to have them not report data. Figure 2.3: The annual number of agencies that report at least one homicide. Figures 2.4 and 2.5 attempt to get at this question by looking the number and percent of all incidents that the top 100, 50 and 10 agencies based on number of homicide incidents make up out of all homicide incidents in each year. These agencies are massively disproportionate in how many homicides they represent - though they are also generally the largest cities in the country so are a small number of agencies but a large share of this nation’s population. On average, the 10 agencies with the most homicide incidents each year - which may change every year - have over 4,000 homicide incidents and make up about 1/4 of all homicide incidents reported nationally. The top 50 have about 7,500 incidents a year, accounting for 46% of incidents. The top 100 agencies have a bit under 10,000 incidents a year and make up over 55% of all homicide incidents in the United States. So excluding the largest agencies in the country would certainly undercount homicides. Figure 2.4: The annual number of homicide incidents, showing all agencies, the top 100 agencies (by number of homicide incidents), top 50, and top 10 agencies, 1976-2022. Figure 2.5: The annual percent of homicide incidents by the top 100 agencies (by number of homicide incidents), top 50, and top 10 agencies, 1976-2022. 2.2 Important variables The data has demographic information for up to 11 victims and 11 offenders, as well as the information on the weapon used by each offender, the relationship between the first victim and each offender, and the circumstance of the homicide. The data also has the traditional SHR set of variables about the agency: their ORI code, population, state, region and the month and year of this data. One key variable that is missing is the outcome of the homicide: there is no information on whether any of the offenders were arrested. While there is information on up to 11 victims and offenders, in most cases, there is only a single victim and a single offender in each incident. We can use the additional_victim_count and additional_offender_count columns to see how many additional victims/offenders there are. An additional victim/offender means in addition to the first one. Even though we have columns for up to 11 victims and offenders, in very rare instances the additional_[victim/offender]_count columns may say there are more than 11 victims/offenders. To see how the breakdown for the number of victims in each incident looks, Figure 2.6 shows the percent of incidents with each possible number of victims.14 In nearly all incidents - 96.0% - there was only a single victim. This drops to 3.3% of incidents for two victims, 0.5% for three victims, and only about 0.2% of incidents have four or more victims. Figure 2.6: The percent of incidents that have 1-11 victims. Figure 2.7 shows the breakdown of the number of offenders per homicide incident.15 It is a little less concentrated than with victims but the vast majority of homicides are committed by one offender - or at least the police only report one offender. About 87.6% of homicides have only one offender, 8.4% have two, 2.5% have three, and 1.5% have four. Fewer than 0.5% of homicides have more than four offenders. However, this is all a bit misleading. In cases where there is no information about the offender, including how many offenders there is, the data simply says that there is a single offender. So the number of homicides with a single offender is an over-count while the number with more offenders is an undercount. Figure 2.7: The percent of incidents that have 1-11 offenders. The variable “situation” says what type of victim-offender number combination the incident is - e.g. “multiple victims/single offender”, “single victim/multiple offenders”, etc. - and does indicate if the number of offenders is unknown (though curiously there are over 4,000 instances where the number of offenders is unknown but they still say there are two offenders) so you can use this variable to determine if the police do not know how many offenders there is. You’re still limited, of course, in that the number of offenders is always what the police think there are, and they may be wrong. So use this variable - and anything that comes from it like the percent of offenders of a certain race - with caution. We will now look at a number of important variables individually. Since the data can potentially have 11 victims and 11 offenders - but in practice has only one each in the vast majority of cases - we will only look at the first victim/offender for each of these variables. Therefore, the results will not be entirely accurate, but will still give you a good overview of the data. The figures below will use data for all homicides from 1976 to 2022 so will cover all currently available years of data. Keep in mind that national trends are not the same as local trends so what is shown in these figures will probably not be the same as what is happening in your community. And that looking at all homicides means we are including murders, manslaughters, and justifiable homicides. 2.2.1 Demographics There are two broad categories of variables that we will cover: demographics of the victim and offenders, and characteristics of the case. We start with demographics. 2.2.1.1 Age This data includes the age (in years) for each victim and each offender. For those under one years old, it also breaks this down into those from birth to six days old “including abandoned infant” and those seven days old to 364 days old. So there is a bit more info on homicides of babies. It also maxes out the age at 99 so for victims or offenders older than that we do not get their exact age, just text that says “99 years or older” (which I turn to the number 99 in the figures below). Figure 2.8 shows the percent of homicides where the first offender in the case is of each age from 0-99. Offenders with unknown ages are excluded from this graph and make up about 27% of cases. The average (mean) age is 31.1 years old (shown in orange) which is due to a long right tail; the median age is 28 years old. If you look closely at the left side of the graph you can see that there are some very young offenders, with at least one offender for each year of age from 0 to 10 included in the data. It is not clear from this alone that these ages are a data entry error. While a two-year-old certainly could not kill someone, the data does include deaths caused by “children playing with gun” (homicide circumstances will be discussed in Section 2.2.2.3) so these ages could potentially be correct. If you are familiar with the age-crime curve in criminology - which basically says crime peaks in late teen years then falls dramatically - this shows that exact curve, though is older and does not decline as the offender ages as quickly as we see with less serious crimes. Figure 2.8: The age of homicide offenders, based on the first offender in any homicide incident. Offenders under age 1 (classified as ‘birth to 7 days old, including abandoned infant’ and ‘7 days to 364 days old’) and considered 0 years old. Offenders reported as ‘99 years or older’ are considered 99 years old. Figure 2.9 repeats Figure 2.8 but with victim age rather than offender age. The mean victim age (shown in orange) is 33 and the median age is 30. Though the average victim age is a bit younger than the average offender age, trends are relatively similar for teenagers and older where deaths spikes in the late teen years and then declines steadily. The major difference is the U-shape for younger victims - for victims under age 15, homicides peak at age 0 (i.e. younger than their first birthday) with ~1.4% of all homicides being this this age. They then decline until plateauing at around age 6 before increasing again in the early teen years. Figure 2.9: The age of homicide victims, based on the first victims in any homicide incident. Victims under age 1 (classified as ‘birth to 7 days old, including abandoned infant’ and ‘7 days to 364 days old’) and considered 0 years old. Victims reported as ‘99 years or older’ are considered 99 years old. 2.2.1.2 Sex We will next look at victim and offender sex, a simple variable since only male and female are included. About 62.2% of offenders, as seen in Figure 2.10, are male and about 8.2% are female, indicating a large disparity in the sex of homicide offenders. The remaining 29.6% of offenders do not have sex data available because the police do not know the sex of this individual. For offenders who are not arrested, this variable may be inaccurate since it is perceived sex of the offender.16 Figure 2.10: The sex of offender #1, 1976-2022. There is far less uncertainty for victim sex, with under 0.17% of victims having an unknown sex. Here again there is a large disparity between male and female with about 78.2% of victims being male and 21.6% being female. Figure 2.11: The sex of victim #1, 1976-2022. 2.2.1.3 Race This data also includes the race of the victims and offenders. This includes the following races: American Indian or Alaskan Native, Asian, Black, Native Hawaiian or Other Pacific Islander, and White. These are the only races included in the data; Hispanic is considered an ethnicity and is available as a separate, though flawed, variable. There is no category for bi- or multi-racial. As with other demographics info for offenders, in cases where no arrest is made (and we do not know in this data if one is made), there is no way to confirm the person’s race so these results may not be entirely accurate. Figure 2.12 shows the percent of homicides in the data by the race of offender #1. Black and White offenders are included are similar percentages, at 34.3% and 33.6% of victims, respectively. The next most common group is Unknown at about 30.6% of offenders. Given that so many offenders have an unknown race, the reliability of race measures is limited. The remaining races are Asian at 0.9% of offenders, American Indian or Alaskan Native at 0.6%, and Native Hawaiian or Other Pacific Islander at 0.02%. Figure 2.12: The race of offender #1, 1976-2022. For victim race, seen in Figure 2.13, only about 1% of victim #1 races are unknown. This means we can be a lot more confident in the race of the victims than in the race of the offender. Similar to offenders, White and Black victims are the two most common races, with 48.4% and 48.1% of victims, respectively. There is a greater share of Asian victims than Asian offenders at 1.5% of victims. American Indian or Alaskan Natives make up 0.8% of victims while Native Hawaiian or Pacific Islanders make up 0.02% of victims. Figure 2.13: The race of victim #1, 1976-2022 2.2.1.4 Ethnicity The final demographic variable is ethnicity which is whether the victim or offender is Hispanic or not Hispanic. The SHR data has a weird relationship with this variable (which is also in the Arrests by Age, Sex, and Race dataset, discussed in Chapter ??) where ethnicity is technically a variable in the data but very rarely collected. As such, this is an unreliable variable that if you really want to use needs careful attention to make sure it is being reported consistently by the agencies that you are looking at. The vast majority - 69.7% - of offenders have an unknown ethnicity while 23.4% are not Hispanic and 7.1% are Hispanic. Figure 2.14: The ethnicity of offender #1, 1976-2022. Unlike the other demographic variables, there is still a huge amount of underreporting when it comes to victim ethnicity, though still less than for offender ethnicity. 55.6% of victims have an unknown ethnicity. Approximately 33.2% of victim #1 are reported as not Hispanic while 11.1% are reported as Hispanic. Figure 2.15: The ethnicity of victim #1, 1976-2022. As an example of agencies under-reporting this variable, let us look at the number of offender #1s in Albuquerque, New Mexico, a city which the US Census says is about 50% Hispanic. Yet the Albuquerque police reported no ethnicity information for almost three decades of data. Figure 2.16: Annual number of offender #1 who is Hispanic in Albuquerque, New Mexico, 1976-2022. 2.2.2 Case characteristics Now we will move to facts about each case such as what weapon was used, how people involved knew each other, and what was the (rough) cause of the homicide. 2.2.2.1 Weapon used The first variable we will look at is the weapon used by each offender. Table 2.1 shows the weapon used by the first offender in every incident from 1976 to 2022. Each offender can only be reported as having a single weapon, so this table essentially shows the number (and percent) of homicides caused by this weapon. This is not entirely true since in reality an offender could use multiple weapons and there can be multiple offenders. In these cases the police include what they believe is the “primary” weapon used by this offender. The most commonly used weapon is a handgun, which is used in nearly half of homicides. This is followed by a knife or other sharp weapon used to cut at almost 15% of homicides, and then by “firearm, type not stated” which is just a firearm where we do not know the exact type (it can include handguns) at 8.9% of homicides The fourth most common weapon is “personal weapons” at nearly 6% of homicides. “Personal weapons” is a weird term to mean that there was no weapon - the “weapon” was the offender who beat the victim to death. Shotguns are involved in almost 5% of homicides and all other weapons are involved in fewer than 5% of cases. In total there are 19 different weapons included though most are very uncommon. Table 2.1: The weapon used in a homicide incident, 1976-2022. In cases where there are multiple offenders, shows only the primary weapon for the first offender. Weapon # of Incidents % of Incidents Handgun 388,178 49.06% Knife Or Cutting Instrument 115,540 14.60% Firearm, Type Not Stated 70,632 8.93% Personal Weapons - Includes Beating 45,473 5.75% Other Or Unknown Weapon 42,002 5.31% Shotgun 36,827 4.65% Blunt Object 34,716 4.39% Rifle 28,108 3.55% Strangulation - Includes Hanging 9,763 1.23% Fire 5,380 0.68% Asphyxiation - Includes Death By Gas 4,804 0.61% Other Gun 3,473 0.44% Narcotics/Drugs - Includes Sleeping Pills 3,144 0.40% Drowning 1,387 0.18% Other Or Type Unknown 586 0.07% Poison - Does Not Include Gas 531 0.07% Explosives 386 0.05% Pushed Or Thrown Out of Window 257 0.03% Narcotics Or Drugs 48 0.01% Total 791,235 100% You may have noticed from the table that AR-15 is not included. While AR-15 is the commonly discussed in the media and policy circles as a way to control gun violence, it is not in a category by itself. Instead it is combined with other rifles in the “rifle” weapon group, and makes up about 3.6% of the weapons used by offender #1 in the data. Let us check if AR-15s, through our rough proxy of the “rifle” weapon group, is getting more common over time. Figure 2.17 shows the number of homicide incidents (including manslaughters, so not necessarily all murders) where offender #1 used a rifle. Figure 2.18 shows the percent of all homicide incidents where the the weapon was a rifle. Using both of these measures we can see the rifles are getting less common, declining substantially since 1980 though increasing again starting in the mid-2010s. Figure 2.17: The annual number of homicide incidents where offender #1’s weapon was a rifle, 1976-2022. Figure 2.18: The annual share of homicide incidents where offender #1’s weapon was a rifle, 1976-2022. Now, maybe this weapon is more commonly used in some types of crimes such as school shootings. You could get at that question using this data by seeing if times when a rifle is used that victims or offenders are younger or if the circumstance is something that suggests a school shooting. Unfortunately there is no offense location variable here, though there is in NIBRS and we can largely recreate this data through NIBRS. And of course you cannot tell if the weapon is actually an AR-15, only if it is a rifle. 2.2.2.2 Relationship between first victim and offenders An interesting and highly useful variable is the relationship between the first victim and each offender. To be clear, this is only for the first victim; we do not have the relationship between other victims and offenders. However, as seen earlier, this is not too much of an issue since nearly all incidents only have a single victim. There are 29 possible relationship types (including “unknown” relationship) which are broken into three categories: legal family members, people known to the victim but who are not family, and people not known to the victim. Table 2.2 shows these relationships and the number and percent of homicides with these relationships. The most common relationship, with about 28% of homicides, is that the police do not know the relationship. So there is a good deal of uncertainty in the relationship between victims and offenders. Next is that the victim is the offender’s acquaintance at 19.7% or is a stranger at 15.3%. The next is “other - known to victim” which is similar to being an acquaintance at almost 5% of homicides. This is followed by the victim being the friend of the murderer at 3.6%. The 6th most common relationship, at 3.6% is that the victim is the wife of the offender, so she is murdered by her husband, and is the first familial relationship of this list. The remaining relationships all make up fewer than 3% of all homicides. Table 2.2: The relationship between the first victim and the first offender in a homicide incident, 1976-2022. Relationship Category # of Incidents % of Incidents Unknown 296,757 37.51% Acquaintance Not family (but known) 156,115 19.73% Stranger Not known 120,719 15.26% Other - Known To Victim Not family (but known) 37,899 4.79% Friend Not family (but known) 28,411 3.59% Wife Family 27,977 3.54% Girlfriend Not family (but known) 21,447 2.71% Husband Family 12,085 1.53% Other Family Family 11,760 1.49% Son Family 11,408 1.44% Boyfriend Not family (but known) 10,009 1.26% Neighbor Not family (but known) 8,081 1.02% Daughter Family 8,056 1.02% Brother Family 6,961 0.88% Father Family 5,667 0.72% Mother Family 5,274 0.67% In-Law Family 4,608 0.58% Common-Law Wife Family 3,317 0.42% Common-Law Husband Family 2,722 0.34% Ex-Wife Not family (but known) 2,365 0.30% Stepfather Family 1,864 0.24% Homosexual Relationship Not family (but known) 1,704 0.22% Sister Family 1,536 0.19% Stepson Family 1,500 0.19% Ex-Husband Not family (but known) 937 0.12% Stepdaughter Family 792 0.10% Employer Not family (but known) 564 0.07% Employee Not family (but known) 451 0.06% Stepmother Family 250 0.03% Total 791,236 100% 2.2.2.3 Homicide circumstance We also have information on the type of the homicide, which this data calls the “circumstance.” This comes as relatively broad categories that leave a lot to be desired in our understanding of what led to the homicide. Table 2.3 shows the number and percent of each circumstance for the first victim of each homicide from 1976 to 2022. This data has 33 possible circumstances which it groups into four main categories: murders that coincide with committing another crime (“felony type” murders), murders that do not coincide with another crime (“non-felony type” murders), justifiable homicides, and negligent manslaughter. The felony type murders are simply ones where another crime occurred during the homicide. While this is called “felony type” it does include other crimes such as theft and gambling (which are not always a felony) so is a bit of a misnomer. The “non-felony type” are murders that happen without another crime. This includes gang killings (where, supposedly, only the murder occurred), children killed by babysitters, fights among intoxicated (both of alcohol and drugs) people, and “lover’s triangle” killings. Justifiable homicides are when a person (civilian or police officer) kill a person who is committing a crime. Negligent manslaughter includes accidental shootings such as when children find and shoot a gun, but excludes deaths from traffic accidents. The most common circumstances, accounting for 27.4%, 26.9%, and 12.5%, respectively, are “Unknown”, “Other Arguments”, and “Other Non-Felony Type - Not Specified.” Since the data includes “Argument Over Money Or Property” as one category, the “Other Arguments” mean that it is an argument for a reason other than over money or property. The “Other Non-Felony Type” one does not mean that the murder did not occur alongside another crime, but also does not fall into the non-felony categories included. Robbery is the only remaining circumstance with more than 5% of murders, at 7.4%. Table 2.3: The circumstance of the homicide for the first offender in a homicide incident. Circumstance Category # of Incidents % of Incidents Unknown 219,450 27.74% Other Arguments Non-Felony Type 212,941 26.91% Other Non-Felony Type - Not Specified Non-Felony Type 98,730 12.48% Robbery Felony Type 58,885 7.44% Narcotic Drug Laws Felony Type 28,360 3.58% Juvenile Gang Killings Non-Felony Type 24,309 3.07% Felon Killed By Police Justifiable Homicide 17,553 2.22% Other Felony Type - Not Specified Felony Type 15,460 1.95% Brawl Due To Influence of Alcohol Non-Felony Type 15,227 1.92% Argument Over Money Or Property Non-Felony Type 14,972 1.89% Felon Killed By Private Citizen Justifiable Homicide 13,928 1.76% All Suspected Felony Type Felony Type 12,975 1.64% All Other Manslaughter By Negligence Except Traffic Deaths Negligent Manslaughter 8,536 1.08% Domestic Violence (Historically Called Lovers Triangle/Quarrel) 6,431 0.81% Burglary Felony Type 6,356 0.80% Gangland Killings Non-Felony Type 5,809 0.73% Brawl Due To Influence of Narcotics Non-Felony Type 4,750 0.60% Lovers Triangle Non-Felony Type 4,158 0.53% Rape Felony Type 4,142 0.52% Other Negligent Handling of Gun Which Resulted In Death of Another Negligent Manslaughter 3,877 0.49% Arson Felony Type 3,304 0.42% Motor Vehicle Theft Felony Type 1,468 0.19% Children Playing With Gun Negligent Manslaughter 1,453 0.18% Other Sex Offenses Felony Type 1,433 0.18% Child Killed By Babysitter Non-Felony Type 1,342 0.17% Institutional Killings Non-Felony Type 1,262 0.16% Gambling Felony Type 1,040 0.13% Larceny Felony Type 916 0.12% Prostitution And Commercialized Vice Felony Type 626 0.08% Other - Not Specified Felony Type 554 0.07% Sniper Attack Non-Felony Type 480 0.06% Victim Shot In Hunting Accident Negligent Manslaughter 350 0.04% Gun Cleaning Death - Other Than Self-Inflicted Negligent Manslaughter 144 0.02% Abortion Felony Type 14 0.00% Human Trafficking/Commercial Sex Acts 1 0.00% Total 791,236 100% 2.2.2.4 Homicide subcircumstance The “subcircumstance” just tells you more information about justifiable homicides. This includes the circumstance leading up to the “felon” - which is how the person killed is described, though technically they do not need to have committed a felony - was killed. It includes if this person attacked an officer (the one who killed them), a different officer, a civilian, or was committing or fleeing a crime. This dataset is one source of information on how many people police kill each year. But it is a large undercount compared to other sources such as the Washington Post collection, so is not a very useful source of information on this topic. Table 2.4: The circumstance for the first offender in a homicide incident in cases where the offender is killed. This includes incidents where the only person who dies in the offender. Subcircumstance # of Incidents % of Incidents Felon Killed In Commission of A Crime 11,026 35.02% Felon Attacked Police Officer 9,224 29.30% Felon Attacked A Civilian 5,499 17.47% Not Enough Information To Determine 2,529 8.03% Felon Resisted Arrest 1,268 4.03% Felon Attacked Fellow Police Officer 1,096 3.48% Felon Attempted Flight From A Crime 840 2.67% Total 31,482 100% CDC WONDER data is available here: https://wonder.cdc.gov/↩︎ 1975 is actually the first year that the Supplementary Homicide Reports data is available but that dataset only has info for a single victim and offender - all later years has info for up to 11 victims and offenders - so 1976 is often used as the first year of data↩︎ There are five incident where there are more than 11 victims. For simplicity of the graph, these incident are excluded.↩︎ There are seven incidents with more than 11 offenders. For simplicity of the graph, these incidents are excluded.↩︎ If we ignore unknown sex, essentially saying that the unknown people will have their sex distributed exactly as the known sex people, 88% are male and 12% are female. However, this assumption is probably wrong since the unknown people may be materially different than the known people, as evidence by them likely not being arrested and committing the crime in a way where even their sex cannot be identified. ↩︎ "],["arsonChapter.html", "Chapter 3 Arson 3.1 Agencies reporting 3.2 Important variables 3.3 Data errors", " Chapter 3 Arson The arson dataset provides monthly counts at the police agency-level for arsons that occur, and includes a breakdown of arsons by the type of arson (e.g. arson of a person’s home, arson of a vehicle, arson of a community/public building) and the estimated value of the damage caused by the arson. We have arson data from 1979 to 2022. This data includes all arsons reported to the police or otherwise known to the police (e.g. discovered while on patrol) and also has a count of arsons that lead to an arrest (of at least one person who committed the arson) and reports that turned out to not be arsons (such as if an investigation found that the fire was started accidentally). This is essentially the Offenses Known and Clearances by Arrest data, detailed in Chapter ??, but only for arsons. The data even follows the same definitions for categories such as what counts as a cleared or unfounded crime. The primary additional variable is the estimated damage in dollars caused by the arson. As much of this information is the same as detailed in Chapter ??, this chapter will be brief. If you have a question about definitions, please refer to that chapter. For each type of arson it includes the number of arsons where the structure was uninhabited or otherwise not in use, so you can estimate the percent of arsons of buildings which had the potential to harm people. This measure is for structures where people normally did not inhabit the structure - such as a vacant building where no one lives. A home where no one is home at the time of the arson does not count as an uninhabited building. In cases where the arson led to a death, that death would be recorded as a murder on the Offenses Known and Clearances by Arrest dataset - but not indicated anywhere on this dataset. If an individual who responds to the arson dies because of it, such as a police officer or a firefighter, this is not considered a homicide (though the officer death is still included in the Law Enforcement Officers Killed and Assaulted data) unless the arsonist intended to cause their deaths. Even though the Offenses Known data uses the Hierarchy Rule, where only the most serious offense that occurs is recorded, all arsons are reported - arson is exempt from the Hierarchy Rule. 3.1 Agencies reporting This dataset measures how many months that an agency reports data over a year in the same way as the Offenses Known data does: the standard FBI definition using the last month reported, and my own measure counting how many months reported data according to a column for each month that says so.17 And just like the Offenses Known data, the variable I use for my own measurement changed in 2018, leading to fewer months of data reported and making it non-comparable to pre-2018 data. The variable changed again in 2021 and said that all agencies always reported data in every month, making the variable useless. In Figure 3.1 we can see the annual number of agencies that reported at least one month of data using both measures. These measures are nearly identical every year with the last month reported having slightly more agencies reported, but they are effectively the same. This changes in 2018 as my measure declines considerably and then skyrockets to nearly 25,000 agencies in 2021 and 2022. The last month reported variable declines considerably in 2021, consistent with the FBI ending SRS collection, and then rebounds in 2022 when the FBI reopens SRS collection. How can there be more than 18k agencies reporting? The 18k number is the estimated number of agencies that are active. Agencies that can respond to crimes and do investigations. Remember that SRS data goes back decades - the Offenses Known data is available since 1930. So agencies can come and go, with agencies shutting down or joining with other agencies. Over time this adds up to thousands of agencies other than the 18k we normal think about. Figure 3.1: The annual number of police agencies that report at least month of data and all 12 months of data that year, 1979-2022. In Figure ?? we can see the same graph as before but now for agencies reporting 12 months of data that year. The trends in both measures are the same, at least until 2018, with the last month reported measure always being a bit higher than the number of months missing measure. Which measure is better to use? In general I would say my own measure but obviously that does not work starting in 2018. In practice both measures are quite similar so the decision on which to use depends on your own use-case such as if you are looking at data after 2017. 3.2 Important variables Similar to the Offenses Known and Clearances by Arrest data, this data shows the monthly number of crimes - in this case only arsons - reported or discovered by the police, the number found by police to be unfounded, and the number cleared by the police. In addition, it includes the number of arsons in structures that were uninhabited, and the estimated cost of the damage caused by the arsons. For each variable, the arsons are also broken into several categories of arsons, which we will talk about in Section 3.2.1. Like other SRS datasets, there are also variables that provide information about the agency - ORI codes, population under jurisdiction - the month and year that the data covers, and how many months reported data. When another agency submits data for the given agency, that is noted in the data through the “covered_by_ori” variable. 3.2.1 Types of arsons For each of the outcome categories detailed below, this dataset has information for ten different types of arson. Some arsons can burn down multiple types or structures or cars - fire, after all, tends to spread. This data defines the arson based on where the fire originated, regardless of what burns after that. This is true even if the damage is more severe for a type other than where the fire started. For example, a recent fire in California was started by a man “pushing a burning car into a ravine.” That fire, known as the Park Fire is still burning as of this writing (summer, 2024) and has burned over 429,000 acres and over 600 buildings. This fire would be classified as a motor vehicle arson because the fire started in a vehicle. There are seven arsons types for buildings, two for vehicles, and one as an “other” category that includes arsons of outdoor areas like parks or forests (though this group does not have any subcategories so all you know is the arson is neither of a building or a vehicle). For both the buildings and the vehicle arson types there is also a “total buildings” and “total vehicles” category that is just the sum of each subcategory; there is also a “grant total” variable that sums all building, vehicle, and other arsons. For each of the building arsons we also have variables that say how many of the arsons were of uninhabited buildings. Total structures (buildings) Single occupancy (e.g. single family homes) Other residential (e.g. hotel, apartment) Storage (warehouses, storage facilities) Industrial Other commercial (e.g. restaurant, office building, car dealership) Community/public (e.g. government buildings, hospitals, community centers, places of worship) All other structures (all buildings that do not fit in another category’) Total mobile vehicles (vehicles) Motor vehicles (a car that runs on a road such as a SUV, sedan, motorcycle) Other mobile vehicles (other mobile objects such as airplanes and boats) Other (everything that does not fit in a previous category) Grand total (all arsons of any category) Some arsons can burn down multiple types or structures or cars - fire, after all, tends to spread. This data defines the arson based on where the fire originated, regardless of what burns after that. This is true even if the damage is more severe for a type other than where the fire started. For example, a (recent fire in California)[https://www.nbcnews.com/news/us-news/man-pushing-burning-car-ravine-started-park-fire-burning-45000-acres-c-rcna163697] was started by a man “pushing a burning car into a ravine.” That fire, known as the Park Fire is still burning as of this writing (summer, 2024) and has burned over 429,000 acres and over 600 buildings. This fire would be classified as a motor vehicle arson because the fire started in a vehicle. 3.2.2 Actual arsons This variable includes the number of arsons either reported to the police or otherwise known to the police in that month and that the police determine to actually be arsons - that is, reports that are not unfounded. This is based only on the police’s determination that an arson occurred, not the decision of other groups like a court or the conviction of someone for the crime. This is the number of arsons, regardless of their severity. An arson that burns down a single vehicle is treated as one arson, as is an arson that burns down a vehicle, 429 thousand acres of land, and 600 buildings. 3.2.3 Unfounded arsons This variable shows the number of arsons reports that the police determined to not actually be arsons. For example, if a house burns down and police think it was arson but later determine that it was an accident, it would be an unfounded arson. 3.2.4 Reported arsons This variable is the sum of actual arsons and unfounded arsons - it is the total number of arsons that were reported or known to the police, even if a later investigation found that it was not actually an arson. Since this is the sum of two already present variables - and is less informative than the two variables are as separate variables - I am not exactly sure why it is included. 3.2.5 Cleared arsons This shows the number of arsons where at least one offender is arrested or when an arrest cannot be made for some reason, but the police know who the offender is - the latter option is called “exceptional clearances” or “clearances by exceptional means.” There is no way to tell if a clearance is due to an arrest or due to exceptional means.18. Please note that this data is at the incident-level which means that having multiple people arrested for an incident still only is a clearance for that single incident. If there are multiple people who committed the arson only one needs to be arrested or exceptionally cleared for the incident to be cleared. Clearances are also reported in the month they occur, not in the month that the initial crime happened. This can lead to cases where there are more clearances than crimes, incorrectly leading to the conclusion that police solve &gt;100% of crimes. Figure 3.2 shows the number of actual arsons (reports that are founded) and clearances for single-family home arsons in League City, Texas, a city of about 100,000 outside of Houston. In most years there were fewer clearances than arsons, but in four years (1982, 1981, 1992, and 2007) there were more clearances than arsons. Figure 3.2: The annual number of single-family home arsons and clearances in League City, Texas, 1979-2022. 3.2.6 Cleared for arsons where all offenders are under age 18 This variable is the same for normal clearances but only for arsons where every offender is under the age of 18. In these cases a clearance by arrest can include citing the juvenile offender with an order to go to court to stand trial, even if the juvenile is not actually taken into police custody. As this variable requires that the police know every offender (to be able to determine that they are all under 18), it is likely flawed and not a very useful variable to use. 3.2.7 Uninhabited building arsons This data also includes the number of arsons that occur in uninhabited structures. These structures must be uninhabited in the long-term, not simply having no one inside them when the arson occurs. The FBI defines these are structures that are “uninhabited, abandoned, deserted, or not normally in use” at the time of the arson. For example, a vacation home whose owners are not living in at the time would be “not normally in use” so would be an uninhabited building. A business that is closed when the fire started, but is open during the day, is not an uninhabited building. 3.2.8 Estimated damage cost The final variable is the estimated cost of the arson. This is how much the police estimates the value (in dollars) of the damaged or destroyed property is. Since this is the value of property damage, injuries to people (including non-physical injuries such as trauma or mental health costs) are not included. Since this is estimated damage it may be inaccurate to some degree. This variable is the sum of monthly estimated cost so while you can get the average cost by dividing this by the number of actual offenses, this average may be significantly off due to having extremely small or large values in your data. This value may be $0 since arsons include attempted arsons which may cause little or no damage. Please note that this value is not inflation-adjusted so you will have to adjust the value yourself. 3.3 Data errors Like the other SRS data, there are some cases where there are obvious data entry errors leading to impossible numbers of reported arsons or the price of an arson. As an example, Figure 3.3 shows the annual number of single-family home arsons in Byron City, Illinois, which has a population of slightly over 3,600 people. Every year with data available there are zero arsons reported until 2003 when 469 arsons are reported. Since it is exceedingly unlikely that suddenly an eighth of the city each suffered different arson attacks, and the city never again had a residence arson, this is almost certainly a data entry error. As arsons are relatively rare, having errors - and especially ones like this - can drastically change the results of your analysis so it is important to check your data carefully for more errors. Figure 3.3: Annual single-family home arsons in Byron City, Illinois. The sudden spike to over 400 arsons in a single year is an example of data errors in this dataset, 1979-2022. There are also cases where it is less clear when a value is a data error or is simply due to an outlier - but real - value. For example, Figure 3.4 shows the annual average cost of a single-family home fire in Los Angeles, California. In most years the value is low. Since an arson can cause little or no damage, these low values likely mean that on average only part of the house was damaged, not the entire house. In 2009, however, the average damage is about $540,000 per arson. Is this a data entry error that simply inputs a damage value that is too high? It certainly appears to be a data error since it is a sudden huge jump in damage value. However, it could also be that some extraordinarily expensive homes were destroyed that year. In 2009 Los Angeles only reported 63 single-family home arsons so having one, or a few, super mansions - which LA has plenty of - destroyed could mean that this huge value is real. Figure 3.4: The annual cost per arson for single family homes in Los Angeles, California, 1979-2022. This is different than identifying how many months have an arson as there may be months that truly do not have any arsons. We do not want to count those are non-reporting months.↩︎ NIBRS data does tell more information about the type of arrest, but SRS data does not↩︎ "],["hate_crimes.html", "Chapter 4 Hate Crime Data 4.1 Agencies reporting 4.2 Tree of Life synagogue shooting 4.3 Important variables", " Chapter 4 Hate Crime Data This dataset covers crimes that are reported to the police and judged by the police to be motivated by hate. More specifically, they are (1) crimes which were (2) motivated - at least in part - by bias towards a certain person or group of people because of characteristics about them such as race, sexual orientation, or religion. The first part is key: incidents must first be crimes—specifically, the types of crimes the FBI includes in this dataset. Actions motivated by bias that do not meet the legal standard of a crime, or fall outside the specific crime categories covered by this data, are not recorded as hate crimes. For example, if someone yells racial slurs at a Black person, it’s clearly a biased and racist action, but it wouldn’t be included in this data unless it involved a specific crime like intimidation. Racial slurs alone, without additional criminal behavior, are generally not illegal and thus wouldn’t be reported as a hate crime in this dataset. For the second part, the bias motivation, it must be against a group that the FBI includes in this data. For example, when this data collection began in 1991, there was no way to collect information about hate crimes against transgender people specifically. Instead it would be counted in the “Anti-Lesbian, Gay, Bisexual, Or Transgender, Mixed Group (LGBT) bias motivation. So if a transgender person was assaulted or killed because they were transgender, there would not be a way to count that until 2013 when anti-transgender was first recorded in this data. In the previous example the offender shouted a racial slur, clear that the actions were motivated by bias. What about a hate crime where there is no explicit evidence of hate? Say, a White man robs a Black man and targets him because he is Black. The offender does not wear any insignia suggesting bias and never says a word to the victim. If the offender is never caught this robbery would not be considered a hate crime as there is no evidence that it is motivated by hate. Even if the offender is caught this would only be considered a hate crime if the police uncover evidence of bias, such as a confession or text messages between the offender and another person explaining why the victim was targeted. I think many - perhaps even most - hate crimes fall into this category. Where it was in fact a hate crime but there is not sufficient evidence - both in terms of evidence the police can gather and even the victim’s own perception - that it was a hate crime. This data is a more limited measure of hate crimes than it may initially appear. It represents only (some) crimes, motivated by (some) types of hate, that are both reported to the police and where the police have gathered sufficient evidence to determine bias. It is also the dataset with the fewest agencies reporting, with most agencies not reporting any hate crimes to the FBI in a given year. This may be true for most agencies as hate crimes are rare and many agencies are small with relatively few crimes of any type reported. However, there is evidence that some agencies that likely have hate crimes still do not report. This leads to gaps in the data with some states having few agencies that report hate crimes, agencies reporting some bias motivations but not others, and agencies reporting some years but not others. While these problems exist for all of the SRS datasets, it is more severe in this data. This problem is further complicated by hate crimes being rare even in agencies that report them. With such rare events, even minor changes in which agencies report or whether victims report the crime to the police can drastically change the reported number of hate crimes. For these reasons I strongly advise caution to anyone using these data. 4.1 Agencies reporting We will start by looking at how many agencies report hate crime each year. This is a bit tricky since there can be multiple ways to examine how agencies report, and since agencies can truly have no hate crimes in a year it is hard to differentiate the true zeroes from the non-reporters. Figure 4.1 shows the number of agencies that report at least one hate crime incident in that year. During the first year of data in 1991 there were about 750 agencies reporting and that grew steadily to about 2,000 agencies in year 2000. From there it increased a bit over the next decade before declining to below 1,750 in the early 2010s and rising again to around 3,000 agencies at the end of our data. Figure 4.1: The annual number of police agencies that report at least one hate crime incident in that year. The 3,000 or so agencies that report each year are not the same every year. Figure 4.2 shows the cumulative number of agencies that have reported at least one hate crime between 1991 and 2022. There is a steady growth in the cumulative number of agencies, with about 350 new agencies each year. In each year some new agencies report hate crimes for the first time while some agencies that reported a hate crime in previous years do not report any hate crimes in the current year. Figure 4.2: The cumulative number of agencies that have reported one or more hate crimes between 1991 and 2022 Figure 4.3 puts this into hard numbers by showing the percent of agencies who reported a hate crime in a certain year who also reported a hate crime in the previous year. For most years between 50% and 60% of agencies which reported a hate crime in the year shown on the x-axis also reported a hate crime in the previous year, indicating somewhat high consistency in which agencies have hate crimes. Figure 4.3: The percent of agencies that report a hate crime in a given year that also reported a hate crime in the previous year, 1992-2022 Another way to understand reporting is to look at the number of reported hate crimes by state and see which states report and which do not. Figure 4.4 does this for 2022 data by showing the number of reported hate crime incidents by state. Unfortunately what we have done here is basically create a population map, though with California as a clear outlier. Counting up and graphing or mapping the number of crimes is a common first response to getting new data but is not actually that helpful. Here we see that the states with the biggest populations - California, New York, Texas, - have the most hate crimes. To be more useful let us look at state-level reporting after adjusting to the number of agencies in the state and to the civilian population. Figure 4.4: Total reported hate crimes by state, 2022 We will start with the rate of agencies reporting though this incorrectly assumes that each agency in the state is comparable. For example, say a state has 10 agencies; one that has jurisdiction over 91% of the state’s population, and nine that have jurisdiction over 1% of the population each. If the one big agency reports and none of the nine do then we will say that only 10% of agencies report data. But this one covers 91% of the state so this is actually great coverage. Conversely, having that one agency not report means that even with the other nine agencies reporting we actually cover less than one-tenth of the state’s population. Still, this is a useful starting point for understanding this data’s reporting and usually answering these kinds of questions requires multiple answers that are all wrong in their own way. Figure 4.5 shows the percent of agencies for each state that reported at least one hate crime in 2022. In New Jersey, the state with the highest percent of agencies reporting, 39% of agencies reported at least one hate crime. It is neighboring states of Pennsylvania, Delaware, and New York have a much lower rate of reporting at 4% (the lowest of any state), 11%, and 14%, respectively. This difference is likely due to a 2019 request by the New Jersey Attorney General to police officers that they https://www.washingtonpost.com/national-security/2022/01/29/hate-crimes-nj-fbi-asian/ To me this suggests that decisions at the state level can lead to drastic changes in reporting rates by agencies, and is a possible solution to low reporting rates. In 15 states, fewer than 10% of agencies reported a hate crime, and in one state (Pennsylvania) fewer than 5% of agencies did so. One interesting finding from this map is the more liberal states - New Jersey, Washington, California, Connecticut, etc. - have the highest share of agencies reporting a hate crime, indicating that the culture of the state may influence either the propensity of hate crimes, whether victims report, whether agencies report hate crimes, or simply that more hate crimes happen in these areas. Figure 4.5: The percent of agencies in each state that reported at least one hate crime in 2022, excluding agencies covered by another agency. To examine how population affects our results, Figure 4.6 shows the percent of each state’s population that is covered by an agency that reported at least one hate crime. Results are similar to Figure 4.5 but now show that there is more reporting than it appeared in that figure. That is because while not all agencies report a hate crime, the ones that do report are generally larger (in terms of population) than the ones that do not. And that is to be expected since smaller agencies will have fewer crimes than larger ones meaning that it is less likely that have a hate crime. So measuring by population we see that about half of the people in the country lives in the jurisdiction of an agency which reported at least one hate crime. The average state also covers about half of the population in a hate-crime-reporting agency. The state with the lowest population covered is Mississippi with 17% of its residents in a jurisdiction with an agency reporting data; the state with the highest share is Hawaii at 86%. Is this good? We do not necessarily want 100% of agencies to report a hate crime since not all agencies will experience a hate crime in their jurisdiction. The ideal dataset would have all hate crimes reported but without knowing how many hates crimes there actually are we cannot tell how well this data captures hate crimes. This is also a fairly poor measure of reporting as it just measures agencies reporting at least one hate crime. If an agency had many hate crimes but only reported very few - and here let us think about that as both agencies not knowing a crime was a hate crime and also knowing but not reporting a hate crime - that is also quite bad for our understanding of hate crimes. However, it is far more likely that a hate crime is not reported than a non-hate crime being reported as a hate crime. Since we know the likely direction of any errors we can think about this entire dataset as being the lower-bound of hate crime data. Figure 4.6: The percent of population in each state in agencies that reported at least one hate crime in 2022, excluding agencies that are covered by another agency. 4.2 Tree of Life synagogue shooting One way I like to check the quality of data is to see how it reports something that I know occurred. Here we will look at how the anti-Semitic attack on a synagogue in Pittsburgh was reported. In October of 2018 the deadliest attack on Jewish people in US history occurred at the Tree of Life synagogue in Pittsburgh, PA. There, 11 congregants were murdered, and several other people, including police officers, were injured by the shooter. Yet according to this data, however, those murders never occurred. Not in Pittsburgh at least. No murders with an anti-Jewish bias were reported in Pittsburgh in 2018. Instead, the shooting was reported by the FBI’s Pittsburgh field office, which, like many federal agencies that have offices across the country, is included in the data as its own agency. This is good and bad. Of course it is good that when a crime happens it is reported in the data. The bad part is that it is counted as hate crimes that occurred in the FBI’s Pittsburgh agency, and not the Pittsburgh Police Department. Most research occurs at the local level - usually studying an agency or county. So if a study is examining agency-level characteristics that are related to hate crimes it’d almost certainly exclude these murders as they are reported by a federal agency and not the local Pittsburgh agency. This also gets complicated as FBI rules say that a crime should be reported by the most local jurisdiction. This is true even when there is overlapping jurisdiction. 11 people were murdered in Pittsburgh, and several Pittsburgh Police officers were injured. That should mean that the crime is reported by Pittsburgh Police, not by the FBI. Pittsburgh does report these murders in their Offenses Known data, making it even more odd that they are Pittsburgh crimes in one dataset and not another.19 4.3 Important variables This data has the standard set of variables describing the agency that is reporting. This includes the agency ORI - which is the unique ID for that agency - the agency name, their state, and the population under their jurisdiction. It then has more detailed information about each crime such as what crime happened, what type of bias it involved, where it occurred, and some demographics of the offender. 4.3.1 Date and time This data says the exact date that the hate crime occurred on - though not the date it was reported on. Figure 4.7 shows the percent of hates crimes between 1991 and 2022 that occurred on each day of the week. Interestingly, the most common days for a hate crime to occur is on Friday, which is also when non-hate crimes most frequently occur. This suggests that hate crimes do follow the same trends - at least partially - as other crimes. Figure 4.7: The day of the week that hate crimes occurred on, 1991-2022 We can also look at which day of the month is most common, as shown in Figure 4.8. There’s no pattern that I can see other than the the 1st of the most has the most hate crimes and the end of the month has the fewest. Not all months have more than 28 days so it makes sense that the 29th, 30th, and 31st are the least common days. Is the 1st of the month really the most dangerous? I think this is likely just a quirk of the data, and is something we also see in NIBRS data. When an agency does not report an actual date they may use the 1st of the month as a placeholder which then looks to us like the 1st is an especially prolific day for hate crimes. Figure 4.8: The day of the month that hate crimes occurred on, 1991-2022 4.3.2 The bias motivation (who the hate is against) The most important variable in this data is the “bias motivation” which is the FBI’s term for the cause of the hate. A hate crime targeted against Black people would be an “anti-Black” bias motivation. For the police to classify an incident as a hate crime, and to assign a particular bias motivation, the police must have some evidence that the crime was motivated by hate. The victim saying that the crime is a hate crime alone is not sufficient - though if large portions of the victim’s community believe that the crime is a hate crime, this can be a factor in the police’s assessment. The evidence required is not major. It includes evidence as explicit as slurs said during an incident and less obvious factors like the crime occurring on an important holiday for that community (e.g. Martin Luther King Day, religious holidays). The FBI also encourages police to consider the totality of the evidence even if none alone strongly suggests that the crime was a hate crime in making their determination about whether the incident was a hate crime or not. This also means that many (perhaps most) hate crimes will not be recorded as hate crimes since there is no evidence that the crime is motivated by hate. Consider, for example, a person who is biased against Asian people and decides to rob them because they are Asian. This is clearly a hate crime. And say this persons robs 10 Asian people in 10 different incidents, causing 10 hate crimes. All of the victims report it to the police but only two of them tell the police that they think it was a hate crime; the other eight do not think it is a hate crime. Without additional information the police would likely not report any of these robberies as hate crimes. And if all ten of the victims happened to be surveyed about crime victimization, such as through the Bureau of Justice Statistics’ National Crime Victimization Survey, only two of the 10 victims would report being the victim of a hate crime. Using FBI data the anti-Asian hate crimes would be zero; using victimization surveys would undercount anti-Asian hate crimes enormously. This is the main problem with using hate crime data, even with perfect reporting or surveys of everyone possibly victimized we may still be getting data that is completely incorrect. In the FBI data bias motivation is based on the offender’s perceptions of the victim so even if they are incorrect in who their victim is, if they intended to target someone for their perceived group membership, that is still a hate crime. For example, if a person assaults a man because they think he is gay, that is a hate crime because the assault was motivated by hate towards gay people. Whether the victim is actually gay or not is not relevant - the offender perceived him to be gay so it is an anti-gay hate crime. To make this even more complicated, the offender must have committed the crime because they are motivated, at least to some degree, by their bias against the victim. Being biased against the victim but targeting them for some other reason means that the crime is not a hate crime. The biases that the FBI includes in this data have grown over time, with new bias motivations being added in 1997, 2012, 2013, and 2015. Table 4.1 shows each bias motivation in this data, the year it was first reported, how many hate crimes there were for this bias motivation from 1991-2022 and what percent of hate crimes that bias motivation makes up. To make the most common bias motivations easier to identify, the table is sorted by the frequency of incidents. The “first year” column reflects the first year that the bias motivation was officially recorded, though some biases may have existed earlier but were not yet captured in the data. The last column in this table shows the percent of hate crime incidents from 1991-2022. This sorting makes it easy to see the most common bias motivations, but that is not actually that useful to most people since we usually care more about a rate than a count. For example, according to this table there were almost three times as many anti-Black hate crimes than anti-Jewish hate crimes, showing that anti-Black hate crimes are more of a problem in this country. But this is not right. We cannot just count of the number of offenses or we risk accidentally just measuring the population of these groups. Black people, for example, make up about 14% of the United States population while Jewish people make up about 2%.20 If we adjust the numbers to equalize population then we see that there is a much higher anti-Jewish hate crime rate than anti-Black rate. And even this is not that useful since you really need a much deeper dive into the data before pulling out these seemingly simple statistics. For example, maybe areas with more Jewish people have better reporting than areas with more Black people. Or that Jewish victims would report to the police at higher rates than Black victims. Maybe these are both true at certain times between 1992 and 2022 but have changed over the years. It is not hard to think of possible explanations for differences between groups so without running down each of these explanations I recommend caution before putting out even something as seemingly simple at the number of crimes by bias group. Table 4.1: The bias motivation for hate crime incidents. In incidents with multiple bias motivation, this shows only the first bias motivation recorded. Bias Motivation First Year Reported # of Incidents % of Incidents Total 240,108 100% Anti-Black 1991 81,208 33.82% Anti-Jewish 1991 29,967 12.48% Anti-White 1991 27,360 11.39% Anti-Male Homosexual (Gay) 1991 23,862 9.94% Anti-Hispanic 1991 15,397 6.41% Anti-Ethnicity Other Than Hispanic 1991 11,498 4.79% Anti-Lesbian, Gay, Bisexual, Or Transgender, Mixed Group (LGBT) 1991 7,815 3.25% Anti-Asian 1991 7,671 3.19% Anti-Multi-Racial Group 1991 5,652 2.35% Anti-Female Homosexual (Lesbian) 1991 4,876 2.03% Anti-Muslim 1991 4,206 1.75% Anti-Other Religion 1991 3,621 1.51% Anti-American Indian Or Native Alaskan 1991 2,781 1.16% Anti-Catholic 1991 1,819 0.76% Anti-Arab 1991 1,510 0.63% Anti-Transgender 2013 1,500 0.62% Anti-Protestant 1991 1,361 0.57% Anti-Mental Disability 1997 1,333 0.56% Anti-Multi-Religious Group 1991 1,314 0.55% Anti-Physical Disability 1997 752 0.31% Anti-Sikh 2015 673 0.28% Anti-Bisexual 1991 652 0.27% Anti-Heterosexual 1991 615 0.26% Anti-Gender Non-Conforming 2012 514 0.21% Anti-Female 2012 443 0.18% Anti-Other Christian 2015 403 0.17% Anti-Eastern Orthodox (Greek, Russian, Etc.) 2015 388 0.16% Anti-Atheism/Agnosticism 1991 201 0.08% Anti-Native Hawaiian Or Other Pacific Islander 2013 184 0.08% Anti-Male 2013 171 0.07% Anti-Mormon 2015 106 0.04% Anti-Hindu 2015 103 0.04% Anti-Buddhist 2015 101 0.04% Anti-Jehovahs Witness 2015 51 0.02% 2015 is the year with the most bias additions, as of data through 2022. This year added a number of religions such as Anti-Buddhist, Anti-Sikh, and Anti-Jehovah’s Witness. In 2013, anti-Transgender was added and this is the most common of the bias motivations added since data began in 1991 with 1500 hate crimes between 2013-2022 - 0.62% of all hate crime incidents from 1991-2022. That year also added anti-male and Anti-Native Hawaiian or Other Pacific Islander, which is the most recent racial group added. In 2012, anti-gender non-conforming and anti-female were included, while in 1997 both anti-mental and anti-physical disability were added. In part due to having fewer years of data available, these newer bias motivations make up a small percent of total hate crimes. The original hate crimes - that is, those in the data in 1991 when this dataset was released - are far more common. The most common bias motivation is anti-Black at 34% of hate crimes, anti-Jewish at 12%, anti-White at 11%, anti-male homosexual (gay) at 10%, anti-Hispanic at 6%, and anti-ethnicity other than Hispanic (this group means a crime against an ethnic group that is not Hispanic, though it is occasionally reported as anti-non-Hispanic which is incorrect.) at 5%. All other bias motivations are less than 5% of hate crimes and consist of a variety of ethnic, racial, religious, or sexual orientation. Some hate crimes can potentially fall in multiple categories. For example, there is a bias motivation of “anti-male homosexual (gay)” and of “anti-lesbian, gay, bisexual, or transgender, mixed group (LGBT)” so there is some overlap between them. When an incident involves multiple bias motivations we can track that in the data as police can report up to 10 bias motivations per incident. In practice, however, most incidents involve only a single bias motivation. 4.3.3 The crime that occurred The “crime” part of hate crimes is which criminal offense occurred during the incident. A hateful act where the action is not one of the crimes that the FBI records would not be considered a hate crime. This is likely most common when considering something like a person calling someone by a hateful slur (e.g. “You’re a [slur],” “go back to your own country”, etc.) but where the action is not technically a crime. Another layer of difficulty in using this data is that not all crimes that the FBI includes were initially included when data become available in 1991. Every several years the FBI adds new crimes to be included in this data. Table 4.2 shows each crime in the data, the first year that this crime was reported, the total number of these crimes reported between 1991 and 2022, and the percent of all incidents this crime makes up.21 Each hate crime incident can cover up to 10 different crimes occurring - for example, a person who burglarizes a synagogue and spray paints a swastika on the wall would have both burglary and vandalism reported in this data. With each crime, this data has the bias motivation for that crime, the location of the crime (in broad categories, not the actual location in the city like a street address would have), and the number of victims for that offense. In practice, in most hate crimes with multiple offenses recorded, the bias motivation, location, and victim count is the same for each offense. Figure 4.9 shows the number of crimes per incident for each hate crime reported between 1991 and 2022. In 96.6% of cases, there is only one offense in that incident.22 This drops sharply to 3.2% of incidents having two offenses, 0.21% having three offenses, 0.019% having four offenses, and 0.002% having five offenses. Even though this data does allow up to 10 offenses per hate crime incident, there has never been a recorded case with more than five offenses. Results are nearly identical when examining the number of bias motivations and locations reported in an incident. Figure 4.9: The number of offenses per hate crime incident. Nearly all hate crimes are vandalism/destruction of property (30%), intimidation (30%), and simple assault (20%) or aggravated assault (11%) with no remaining crime making up more than 2% of total hate crimes. Table 4.2: The offense type for hate crime incidents. In incidents with multiple offense types, this shows only the first offense type recorded. Offense First Year Reported # of Incidents % of Incidents Total 240,108 100% Destruction of Property/Vandalism 1991 72,489 30.19% Intimidation 1991 71,583 29.81% Simple Assault 1991 47,917 19.96% Aggravated Assault 1991 26,879 11.19% Robbery 1991 4,339 1.81% Burglary/Breaking And Entering 1991 3,890 1.62% All Other Larceny 1993 2,584 1.08% Arson 1991 1,456 0.61% Drug/Narcotic Violations 1993 1,380 0.57% Theft-Other 1991 917 0.38% Theft From Motor Vehicle 1993 884 0.37% Shoplifting 1993 771 0.32% Theft From Building 1994 617 0.26% Motor Vehicle Theft 1992 577 0.24% Weapon Law Violations 1993 469 0.20% Drug Equipment Violations 1995 391 0.16% False Pretenses/Swindle/Confidence Game 1997 353 0.15% Murder/Non-Negligent Manslaughter 1991 330 0.14% Forcible Rape 1991 314 0.13% Theft of Motor Vehicle Parts/Accessories 1993 249 0.10% Counterfeiting/Forgery 1993 245 0.10% Forcible Fondling - Indecent Liberties/Child Molest 1993 225 0.09% Credit Card/Atm Fraud 1995 182 0.08% Impersonation 2001 152 0.06% Kidnapping/Abduction 1994 152 0.06% Stolen Property Offenses - Receiving, Selling, Etc. 1996 140 0.06% Fraud-Other 2016 103 0.04% Forcible Sodomy 1995 82 0.03% Pornography/Obscene Material 1995 81 0.03% Embezzlement 1995 66 0.03% Extortion/Blackmail 1997 62 0.03% Sexual Assault With An Object 1996 41 0.02% Purse-Snatching 1995 29 0.01% Pocket-Picking 1996 28 0.01% Wire Fraud 2006 26 0.01% Statutory Rape 1999 21 0.01% Theft From Coin-Operated Machine Or Device 1999 16 0.01% Unknown 2018 15 0.01% Prostitution 2001 14 0.01% Negligent Manslaughter 1999 8 0.00% Welfare Fraud 1996 8 0.00% Incest 1997 7 0.00% Assisting Or Promoting Prostitution 2013 5 0.00% Human Trafficking - Commercial Sex Acts 2017 4 0.00% Bribery 2014 4 0.00% Purchasing Prostitution 2013 2 0.00% Human Trafficking - Involuntary Servitude 2021 1 0.00% Agencies that report to the FBI’s National Incident-Based Reporting System (NIBRS) can also report bias motivations for their crimes, and these reports are included in this dataset. One tricky thing is that the crimes included are different depending on if the agency reported through NIBRS or to the dataset directly, and are not NIBRS reporting agencies. NIBRS agencies report all of the crimes as the agencies directly submitting SRS data, but have a wider variety of crimes they can report. In practice, however, both NIBRS and SRS reporting agencies can report the most common offenses so there is relatively little difference. 4.3.4 The location of the crime This data is interesting because it includes the location - in categories for types of places, not actual addresses - of the incident. This is important since the type of location can be a factor in whether the incident is classified as a hate crime. For example, spray paint on a synagogue or a mosque is much more likely to be a hate crime than spray paint on a wall of an abandoned building. Table 4.3 shows the locations of hate crimes, including the first year that location was reported. Each hate crime incident can have multiple locations (up to ten) since each offense can have its own incident, but in most cases (96.6%) a hate crime only has a single location. As with the crime and the bias motivation, the available locations have increased as time went on, though these newer locations are relatively uncommon. One important change in location is that starting in 2010 the location of “school/college” was split to have one location be for elementary and high schools and another location be for colleges and universities. The majority of hate crimes occur in the victim’s home (30%), on a road or alley (19%), in an other or unknown location (13%), and in a parking lot or parking garage (6%). All other locations occur in fewer than 5% of hate crimes. Table 4.3: The location of hate crime incidents. In incidents with multiple locations, this shows only the first location recorded. Location First Year Reported # of Incidents % of Incidents Total 239,665 100% Residence/Home 1991 70,736 29.51% Highway/Road/Alley 1991 45,004 18.78% Other/Unknown 1991 31,218 13.03% School/College 1991 17,550 7.32% Parking Lot/Garage 1991 13,786 5.75% Church/Synagogue/Temple 1991 8,926 3.72% Commercial/Office Building 1991 5,403 2.25% Restaurant 1991 5,099 2.13% Bar/Nightclub 1991 4,088 1.71% School - Elementary/Secondary 2010 3,919 1.64% Government/Public Building 1991 3,545 1.48% Convenience Store 1991 3,357 1.40% Specialty Store - Tv, Fur, Etc. 1991 2,795 1.17% Air/Bus/Train Terminal 1991 2,531 1.06% Service/Gas Station 1991 2,287 0.95% Grocery/Supermarket 1991 2,178 0.91% Field/Woods 1991 2,169 0.91% School - College/University 2010 2,068 0.86% Department/Discount Store 1991 2,024 0.84% Drug Store/Doctors Office/Hospital 1991 1,967 0.82% Park/Playground 2010 1,883 0.79% Jail/Prison 1991 1,726 0.72% Hotel/Motel 1991 1,618 0.68% Construction Site 1991 643 0.27% Bank/Savings And Loan 1991 556 0.23% Liquor Store 1991 459 0.19% Lake/Waterway 1991 416 0.17% Shopping Mall 2010 268 0.11% Rental Storage Facility 1991 257 0.11% Community Center 2013 215 0.09% Shelter - Mission/Homeless 2011 169 0.07% Industrial Site 2010 135 0.06% Arena/Stadium/Fairgrounds/Coliseum 2011 88 0.04% Auto Dealership New/Used 2011 82 0.03% Camp/Campground 2010 77 0.03% Abandoned/Condemned Structure 2011 75 0.03% Gambling Facility/Casino/Race Track 2010 69 0.03% Rest Area 2011 65 0.03% Dock/Wharf/Freight/Modal Terminal 2012 47 0.02% Daycare Facility 2011 47 0.02% Amusement Park 2011 41 0.02% Farm Facility 2011 34 0.01% Tribal Lands 2011 23 0.01% Atm Separate From Bank 2011 17 0.01% Military Installation 2015 5 0.00% 4.3.5 Number and race of offenders There are two variables that have information about the people who commit the hate crime: the number of offenders and the race of the offenders. The offender race is recorded as a single value with the race of the group if all are of the same race or it will say a “multi-racial” group if there are offenders of different races. Unfortunately, important information like the age of the offenders, their criminal history, their relationship to the victim, their gender, or whether they are arrested are completely unavailable in this dataset. These variables, however, are available in NIBRS data. As shown in Figure 4.11, the most common racial group is “unknown” since the police do not know the race of the offenders. Next are White offenders at nearly 40% of hate crimes followed by Black offenders at nearly 13% of hate crimes. The remaining racial groups are rare with about 2% of hate crimes being committed by a multi-racial group of offenders and 0.8 % of hate crimes committed by Asian or Pacific Islander offenders and 0.6 % committed by American Indian or Native Alaskan offenders. Only 0.05% of offenders are Native Hawaiian or Other Pacific Islander. Figure 4.10: The race of offenders, as a group, for hate crime incidents, 1991-2022. When the police do not have any information about the number of offenders (which is common in cases of property crimes such as vandalism but rare in violent crimes), this data considers that to have zero offenders. The zero is just a placeholder that means that the police do not know how many offenders there are, not that they think there were actually no offenders. Figure 4.11 shows the percent of hate crimes from 1991-2022 that have each number of offenders recorded. In the actual data it says the actual number of offenders, with the largest group in the current data going to 99 offenders - in this graph I group 10 or more offenders together for simplicity. I also relabel zero offenders as “Unknown” offenders since that is more accurate. The most common number of offenders per hate crime is one offender, at about 49% of hate crimes from 1991-2022 having only one offender. This drops sharply to 9% of hate crimes having two offenders and continues to drop as the number of offenders increase. However, about a third (36%) of hate crimes have an unknown number of offenders. Figure 4.11: The race of offenders, as a group, for hate crime incidents, 1991-2022. 4.3.6 Number of victims When considering the data itself, hate crime data is very similar to most other datasets. It is just the number of crimes that were reported to the police, though with the additional step of having evidence of bias. But the difference in use is that while in other crimes the victim is usually, well, the victim, in hate crimes the victim may be a much wider group. Consider a burglary: the homeowner is the direct victim, as their property was targeted. However, the crime can also affect their neighbors, who may now feel unsafe in their own homes, as well as the victim’s family, who may worry about their safety. Hate crimes, however, tend to affect not just the direct victim but also the entire targeted group, or at least a broader community. A swastika painted on a synagogue, for example, impacts not only the synagogue’s congregation but may instill fear in the broader Jewish community. If a swastika, for example, is spray painted on the front door of a synagogue, who is the victim? Directly it’d be whomever owns the synagogue building. But it also affects all members of that congregation. And what about members of other synagogues in the city? What about Jewish people in the city who do not go to synagogue? Even though only a single crime occurred - a vandalism - it is very difficult to count how many victims there were. Is a swastika on a synagogue worse if the synagogue has a small congregation versus a large one? What if it is in a city with only one synagogue compared to a city with many? Is it worse to have a large swastika than a small one? If we are trying to use this data to measure hate against a particular group these are questions we need to answer, but are of course impossible to answer with this data. Remember, all of the FBI data is essentially just abstract contextless numbers in a spreadsheet. This is true for all UCR data but especially so for hate crimes where no two hate crimes are equal. One burglary in City A is about equivalent to one burglary in City B. For hate crimes a single incident may affect far more people in City A than in City B. In fact, I would argue that this issue is bad enough that we should be extraordinarily cautious when using this data. Just aggregating up the number of incidents is insufficient to measuring either hate or fear. Sure, you can measure the number of hate crimes reported to the police and where the police found adequate evidence to label the crime as bias motivated. But is that really what you want be measuring when using hate crime data? Nonetheless, this is a book about the data. So let us look at one final variable in this data, the number of victims for each incident. This is not going to be true number of people affected by the crime. It is more the number of direct victims for the incident. Whether that is actually better than just counting incidents is dependent on the context of your question and the hate crimes in question. In Figure 4.12 I show the annual number of anti-Jewish hate crimes reported by all agencies in the country. As may be expected, there are always more victims than incidents though the trends are extremely similar over the entire period of data. This trend is also present for other bias motivations, such as anti-Black hate crimes shown in Figure 4.13. While this variable is available in the data, I actually think it is best not to use it. I think there is always a danger in being overly precise and, therefore, overly confident about what the data shows. When you use the number of incidents you implicitly allow for each incident to affect multiple people23 and readers understand that. But if you use this variable and say that “this is the number of victims of this crime” you are implicitly closing that door and therefore being too confident about how many victims of a crime there is. This is especially true for readers who are not paying close attention - such as academics reviewing papers or New York Times reporters - since they may think you are measuring the number of victims in a better way than you actually are. Figure 4.12: The annual number of anti-Jewish hate crime incidents and victims in the United States, 1991-2022. Figure 4.13: The annual number of anti-Black hate crime incidents and victims in the United States, 1991-2022. The murders of nine Black parishioners in the Emanuel African Methodist Episcopal Church in Charleston, South Carolina, in 2015 was reported by the Charleston Police Department, making it even more inconsistent for when the FBI reports hate crime murders.↩︎ For simplicity I am treating these groups as independent though of course some Black people can be Jewish.↩︎ This tables uses only the first offense in an incident so counts are slightly lower than if all crimes in every incident is used.↩︎ In 0.0004% of hate crimes there is no recorded offense. This is not shown in the graph.↩︎ One of the points of hate crimes is to cause fear in more than just the direct victim of the crime.↩︎ "],["administrative-segment.html", "Chapter 5 Administrative Segment 5.1 The incident and report date 5.2 Hour of incident 5.3 Exceptional clearance 5.4 Number of other segments", " Chapter 5 Administrative Segment The Administrative Segment provides information about the incident itself, such as how many victims or offenders there were. In practice this means that it tells us how many other segments - Offense, Offender, Victim, and Arrestee Segments - there are for this particular incident. It also has several important variables at the incident-level such as what hour of the day the incident occurred and whether the “incident date” variable refers to the date the incident occurred or the date it was reported to the police. Finally, it tells us whether the case was cleared exceptionally and, if so, what type of exceptional clearance it was. An exceptional clearance is one where the police can declare the case closed but without making an arrest. This can tell us, for example, how many crimes were cleared because the offender died or the victim refused to cooperate. In comparison, the Offenses Known and Clearances by Arrest data that is part of the SRS and is detailed in Chapter ??, tells us how many offenses were cleared by either arrest or exceptional clearance, but does not differentiate between the two. NIBRS, therefore, provides a deeper understanding of case outcomes. In addition to the variables detailed below, this segment has the traditional agency and incident identifiers: the ORI code, the agency state, the year of this data, and the incident number. 5.1 The incident and report date An important variable, especially for policy analyses, is when the crime happened. NIBRS tells you both the date and the hour of the day for when the crime occurred. We will start with the date. We can convert the date a few different ways, such as daily, weekly, monthly, quarterly. We could use this precise date to do regression discontinuity studies where we look at days just before and just after some policy change or natural experiment. In this chapter we will look simply at the percent of crimes each month and each day of the month (overall, not within each month). And we will look at all incidents; if you want to see the distribution for certain offenses or victim/offender groups you will need to merge this segment with one of the other segments. Figure 5.1 shows the percent of incidents in the 2022 for each month. Past research has found that crimes are lowest when it is cold and highest when it is hot24. Consistent with previous research, we find that crime rates are lowest in February, steadily increasing through the warmer months before peaking in July and August, then decreasing as temperatures cool. These seasonal patterns are important in understanding how environmental factors, such as weather, influence criminal activity, and they can help law enforcement agencies plan how many officers they want on patrol since they can determine which times of the year have the highest expected crime. Figure 5.1: The percent of crime incidents in 2022 NIBRS by the month of incident. We can also look at the days of the month to see if there is any variation there. Figure 5.2 shows the percent of incidents on each day of the month. There’s not much variation other than a few days. The 29th and 30th day of the month have fewer incidents than average, and the 31st day has by far the fewest incidents These findings are reasonable since not all months have more than 28 days so by definition there are fewer 31st (and 29th, and 30th) days of the month for crimes to occur on. The most common day of the month is the 1st which accounts for 3.95% of all incidents. In this data the agencies must report a date, even if they do not know the exact date; there is no option to put “unknown date”. When agencies are unsure of the exact date of a crime, they appear to default to entering the 1st of the month as a placeholder. This practice introduces a potential source of error, and researchers should be cautious when analyzing trends that rely on specific dates, as the 1st of the month may disproportionately represent incidents with unknown dates. Figure 5.2: The percent of incidents that occur (the day of the incident, even if the incidents was not reported that day) each day of the month for all agencies reporting to NIBRS in 2022. The above graph showed the days of the month where the incident was said to occur. There is also a variable that says if the date included was the incident date or the date the crime was reported to the police. Figure 5.3 replicates Figure 5.2 but now shows only report dates rather than incident date. Here too we see the same pattern of the 1st of the month having a disproportionate share of data, again suggesting that it is a placeholder for “unknown” dates. Figure 5.3: The percent of incidents that are reported (the day of the report, even if not the day of the incident) each day of the month for all agencies reporting to NIBRS in 2022. 5.2 Hour of incident Understanding the exact time of day when crimes occur is crucial for developing effective anti-crime strategies. For example, if crime spikes consistently at the end of the local high school day, this may indicate that students are involved in these incidents, either as victims or offenders. Law enforcement agencies can use this information to adjust patrol schedules and allocate resources more effectively to areas and times with higher crime rates. Luckily, NIBRS data does have the time of each incident, though it is only at the hour level. Figure 5.4 shows the distribution of incidents that occurred in the 2022 for each hour of the day. There are two key trends in this figure. First, past research has found that crime tends to increase during the night (this is especially true during weekends), drop to a daily low in the couple of hours before sunrise, and then slowly increase as the day progresses.25 What we find here is a little different. Crime peaks at night at 5-5:59pm which is far earlier than other estimates. Since this is all crimes it could be biased by large increases of certain crimes at this time, such as people coming home from work and finding their house burgled. As crimes differ in their timing (e.g. burglary happens often during the day, fights are more common at night), you will need to merge this segment with the Offense Segment to be able to look at certain types of crimes alone. The substantial spike at midnight is unlikely to reflect actual crime patterns, as the number of incidents during this hour is more than triple that of neighboring hours. The noon hour is about 50% larger than in the neighboring hours, so is a sizable increase though continues the trend of increasing crime during the day and is a far smaller increase than at midnight. This suggests that, similar to the “1st of the month” issue, officers may be using midnight and (less so) noon as a placeholder when the exact time of the crime is unknown. Researchers should exclude the midnight and noon hours from time-sensitive analyses to avoid skewed results. Figure 5.4: The percent of crimes that are reported each hour for all agencies reporting to NIBRS in 2022. To look at these trends over time, Figure 5.5 shows the percent of incidents each year that are reported at noon, at midnight, and where the hour is unknown. The noon hour has slowly and steadily become more common, moving from about 4% in 1991 to 6% in 2022. The midnight hour has seen more fluctuations, increasing to 9% by 1993 before steadily decreasing until a large and sustained spike to 9% in 2017. The spike was caused by the end of data being reported as having an unknown hour. While the share of incidents with an unknown hour has also fluctuated - from around 2.5% to 5% of incidents depending on the year - that dropped to 0% in 2017, as unknown hours stopped being reported after 2016. Figure 5.5: Annual percent of incidents that occurred at midnight, noon, and at an unknown time, 1991-2022. Another way to visualize this is to see what hour is most and least common for every year we have data, as shown in Table 5.1. Results are strikingly similar for the entire time period we have NIBRS. In every year except for 1991 the most common hour is midnight, and in every year the least common is 5am. When excluding midnight the most common hours are are the end of the work day at 5PM-5:59PM and 6PM-6:59PM, or at noon. NIBRS data is available since 1991, and the number of agencies reporting has grown each year. This is also a time period which has seen considerable changes in crimes, an increase in the 1990s followed by a sustained decrease since then until a (now seemingly temporary) spike starting in 2020. Yet throughout all these changes the most and least common hours remain very consistent, suggesting that there appear to strong rules of when crime occurs regardless of other changes. Or at least strong rules in what appears in our data, as I do not believe the midnight or noon hour results are real. Table 5.1: The most and least common incident hours, and the most common hours excluding midnight and noon. Year Most Common Least Common Most Common, Exclude Midnight Most Common, Exclude Midnight/Noon 1991 6PM 5AM 6PM 6PM 1992 Midnight 5AM 6PM 6PM 1993 Midnight 5AM 6PM 6PM 1994 Midnight 5AM 6PM 6PM 1995 Midnight 5AM 6PM 6PM 1996 Midnight 5AM 6PM 6PM 1997 Midnight 5AM 6PM 6PM 1998 Midnight 5AM 6PM 6PM 1999 Midnight 5AM 5PM 5PM 2000 Midnight 5AM 5PM 5PM 2001 Midnight 5AM 5PM 5PM 2002 Midnight 5AM 5PM 5PM 2003 Midnight 5AM 5PM 5PM 2004 Midnight 5AM 5PM 5PM 2005 Midnight 5AM 5PM 5PM 2006 Midnight 5AM 5PM 5PM 2007 Midnight 5AM 5PM 5PM 2008 Midnight 5AM 5PM 5PM 2009 Midnight 5AM Noon 5PM 2010 Midnight 5AM Noon 5PM 2011 Midnight 5AM Noon 5PM 2012 Midnight 5AM Noon 5PM 2013 Midnight 5AM Noon 5PM 2014 Midnight 5AM Noon 5PM 2015 Midnight 5AM Noon 5PM 2016 Midnight 5AM Noon 5PM 2017 Midnight 5AM Noon 5PM 2018 Midnight 5AM Noon 5PM 2019 Midnight 5AM Noon 5PM 2020 Midnight 5AM Noon 5PM 2021 Midnight 5AM Noon 5PM 2022 Midnight 5AM Noon 5PM 5.3 Exceptional clearance When we speak of clearances we generally mean that a person was arrested for the crime.26 Cases may also be cleared “through exceptional means” which is also called an “exceptional clearance.” Exceptional clearances, which occur in about 3% of cases, are important because they indicate that the police have identified the offender and have enough evidence to arrest them, but are unable to do so for specific reasons. Unlike standard clearances, which involve arrests, exceptional clearances may result from factors such as the offender’s death, the victim’s refusal to cooperate, or the prosecution’s decision not to pursue the case. Basically, if they could arrest them they would but for some reason they cannot. NIBRS data tells us if the case is exceptionally cleared as well as the reason for the exceptional clearance. Figure 5.6 shows the breakdown of reasons why the case was cleared for these ~3 of cases that are exceptionally cleared. The most common reason, at 45% of exceptional clearances, is that the victim refused to cooperate with the police or prosecution. This can include cases where the victim reported their crime to the police and then later decide to stop assisting. The next most common reason, also at at 45% of exceptionally cleared cases, is that the prosecution decides to not prosecute the case. This excludes cases where the prosecution believes that there is not probable cause to make the arrest. Therefore it largely includes cases that the prosecution either does not believe they could win or where the agency has a policy of non-prosecution - this is likely increasingly common in jurisdiction which has “progressive prosecutors” who de facto legalize certain crimes. Next we have when the offender is a juvenile and the police chose to avoid arresting them due to their age, which makes up about 7% of these incidents. This generally occurs in minor offenses such as property crimes and the police do give notice to the juvenile offender’s parents or guardians about the situation. If the offender is in another agency’s jurisdiction (including out of the country) and the agency reporting data is unable to make an arrest, including when the agency who has the offender in their jurisdiction refuses to extradite the offender, the case can be exceptionally cleared. This occurs in 2% of exceptional clearances. In these cases we do not know any information about which jurisdiction the offender is in at the time of the exceptional clearance. Finally, if the offender dies (by any means) before the arrest they cannot be arrested so the case is exceptionally cleared. This makes up about 1% of exceptional clearances. The values shown in Figure 5.6 are for all incidents so can be quite different when examining subsets of the data such as by offender demographics or incident type. Doing this would require merging the Administrative Segment with another segment such as Offense or Victim. Figure 5.6: The distribution of exceptional clearances for all exceptional clearances reported to NIBRS in 2022. In Figure 5.7 we can see trends in the percent of incidents that involve an arrest or an exceptional clearance. Ignoring the spike in the arrest rate in the first few years of data, likely part of growing pains of any new dataset, the share of incidents with an arrest is relatively steady over time, increasing until it peaks at a little under 30% of incidents in the mid-2010s and then declining since then. The share of incidents that are exceptionally cleared likewise are relatively steady but do show a slow decline over time, moving from a bit over 5% at the start of our data to about 3% by the end. These changes may simply be due to different agencies reporting over time but they are steady enough that I think the trend likely accurately reflects arrest and exceptional clearance rates in the US. Figure 5.7: Percent of incidents with an arrest or exceptional clearance, 1991-2022. 5.4 Number of other segments The “Administrative” part of this segment is that it tells us about other segments related to this incident. Here we know how many Offense, Offender, Victim, and Arrestee segments there are for the incident. In effect it says how many crimes were committed, how many offenders involved (at least the number known to police), how many victims there were, and how many people were arrested for this particular incident. This can be useful both as a check to make sure you are not missing anything when looking at the other segments and to quickly subset data, such as to only single-victim or only multiple-offender incidents. 5.4.1 Offense Segments This variable indicates how many offense segments there are associated with this incident. Each incident can have multiple offenses, so this just says how many of these crimes there were. For example, if a person is raped and robbed then there would be two offense segments related to that incident. Figure 5.8 shows the number of offense segments - and thus the number of crimes - associated with each incident. The vast majority of incidents only have one offense reported, making up 88% of incidents.27 This drops considerably to 10% of incidents having two offenses, 1% having three, and then under 0.15% of incidents having four through nine offenses. Figure 5.8: The distribution for the number of Offender Segments per incident, for all incidents in NIBRS 2022. This trend is consistent over time. As shown in Figure 5.9, the median number of offense segments each year is one, while the mean number is slightly over one. Figure 5.9: Annual mean and median number of Offense Segments, 1991-2022. 5.4.2 Offender Segments The Administrative Segment tells you how many offenders are involved with an incident. This is, of course, an estimate because in some incidents the police do not know how many people are involved. If, for example, someone was robbed then they can tell the police how many robbers there were. But if someone comes home to find their home burglarized then they do not know how many burglars there were. If there is no video evidence (e.g. a home security camera) and neighbors did not see anything then the police would not know how many offenders were involved in the incident. In these cases they put in a single offender and in the Offender Segment all of the information about the offender is “unknown.” The remaining number of offenders are still estimates as the police may not know for sure how many offenders were involved, but this is more reliable than when there is only a single offender reported. With that major caveat in mind, Figure 5.10 shows the distribution in how many offenders there were per incident. The vast majority of incidents have only one (or potentially an unknown number) offenders, at 91% percent of incidents. Incidents with two offenders make up only 7% of incidents while those with three make up 1% of incidents. No other number of offenders make up more than 0.5% of incidents. The data does have the exact number of offenders but I have top coded it to 10 in the graph for simplicity. There can potentially be a large number of offenders involved in an incident and in the 2022 NIBRS data the incident with the higher number of offenders had 86. However, it is exceedingly rare for there to be even more than a handful of offenders. Figure 5.10: The distribution for the number of Offender Segments per incident, for all incidents in NIBRS 2022. As seen in Figure 5.11, in every year the median number of offenders is one and the mean number is just above one. Figure 5.11: Annual mean and median number of Offender Segments, 1991-2022. 5.4.3 Victim Segments In cases where the offense is a “victimless crime” (or at least one where there is no specific victim) such as a drug offense, the victim and the offender can be the same individual. For other cases, being a victim in an incident just means that you were the victim of at least one offense committed in the incident. Figure 5.12 shows the distribution in the number of victims per incident. Like the number of offenses and offenders, this is massively skewed to the left with 91% of incidents having a single victim. Incidents with two victims make up 8% of the data while incidents with three victims are 1%. All remaining numbers of victims are less than one third of 0.5% of the data each. The data does have the exact number of victims but I have top coded it to 10 in the graph for simplicity. The incident with the most victims in 2022 had 163 victims. Figure 5.12: The distribution for the number of Victim Segments per incident, for all incidents in NIBRS 2022. Similar to what we have seen with offenses and offenders, we can see in Figure 5.13 that the median number of victims is one and the mean number is just a bit more than one. Figure 5.13: Annual mean and median number of Victim Segments, 1991-2022. 5.4.4 Arrestee Segments Unlike the previous segments, there may not always be an arrestee segment since not all crimes lead to an arrest. Figure 5.14 shows the distribution in the number of arrestee segments per incident in the 2022 NIBRS data. Indeed, the vast majority - 77% of incidents - did not lead to a single arrest. In 21% of incidents a single person was arrested while in 2% of incidents two people were arrested. The remaining numbers of people arrested are increasingly small with fewer than 0.3% of incidents having more than three people arrested. The incident with the most arrests in 2022 led to 65 people arrested. Figure 5.14: The distribution for the number of Arrestee Segments per incident, for all incidents in NIBRS 2022. Of course, to really understand these arrests we would need to know how many people committed the crime. Having one arrest for an incident with one offender is good, having one arrest when there are multiple offenders means some criminals are walking free. While we do not know the true number of offenders (as police may not know how many there actually were), we can use the Offender Segment count as an estimate. Figure 5.15 shows the percent of incidents where at least one offender was arrested and where all offenders were arrested, broken down by the number of reported offenders. There is wide variability in the percent of offenders arrested by the number of offenders in an incident. In cases with one offender, there was an arrest made only 22% of the time. Given that this includes incidents without a known offender, I expect the true arrest rate for incidents that actually have one offender to be higher. When there are two offenders, about 39% of incidents have at least one arrest and 26% of incidents have both offenders arrested. For having at least one person arrested we see a fairly steady rate of mid- to high-30% for each number of offenders. In contrast, the share of incidents where all offenders are arrested declines with each additional offender, reaching to only 9% with 10 or more offenders. Figure 5.15: The percent of incidents by number of offenders where at least one offender is arrested and where all offenders are arrested. The median number of arrestee segments over time, as shown in Figure 5.16 is zero, with the mean number slightly higher at around 0.3. Figure 5.16: Annual mean and median number of Arrestee Segments, 1991-2022. In summary, the Administrative Segment provides a useful metadata for understanding what other segments are available for each incident. Although it is often necessary to combine this data with other segments to gain a full understanding of the incident, the information in the Administrative Segment - such as the timing of the crime and exceptional clearance details - offers useful insights into the broader patterns of criminal activity and law enforcement responses. Summer also comes with many teens and young adults out of school so have more free time to offend or be victimized, so the weather is only part of the cause.↩︎ In all of the nighttime police ride-alongs I have been on the police tend to stop patrolling in early morning (e.g. 3-4am) and go back to the station to do paperwork. I think this likely partially explains the findings that crime is lowest around 4-5am.↩︎ While a more expansive definition may include a conviction in a court for that crime (including pleading guilty), NIBRS data only extends to the arrest stage so we never know the judicial outcome of the case.↩︎ In reality a person who commits a crime may be arrested or charged with many (often highly related) offenses related to a single criminal incident. So this data does report fewer incidents than you would likely find in other data sources, such as if you request data from a local police agency or district attorney’s office.↩︎ "],["arrestee.html", "Chapter 6 Arrestee and Group B Arrestee 6.1 Crimes arrested for 6.2 Arrest date 6.3 Weapons 6.4 Automatic weapons 6.5 Type of arrest 6.6 Disposition for juvenile arrestees 6.7 Demographics", " Chapter 6 Arrestee and Group B Arrestee The Arrestee Segment has information on the person arrested in an incident and has a number of variables that look at same as in previous segments but with subtle differences. This segment covers the arrestee’s age, sex, and race, ethnicity, and residency status (of the city, not as a United States citizen). Age, sex, and race are also in the Offender Segment but can differ as not all offenders are arrested. It also says the crime the arrestee was arrested for (which in some cases is different than the crime committed in the offense since an arrest can clear multiple incidents), the weapon carried during the arrest (which may be different than the weapon used during the offense) and if this weapon (if it is a firearm) was an automatic weapon. There are a few completely new variables including the date of the arrest and the type of arrest. The type of arrest is simply whether the person was arrested by police who viewed the crime, if the arrest followed an arrest warrant or a previous arrest (i.e. arrested for a different crime and then police find out you also committed this one so they consider you arrested for this one too), and whether the person was cited by police and ordered to appear in court but not formally taken into custody. Finally, for juvenile arrestees it says whether arrestees were “handled within the department” which means they were released without formal sanctions or were “referred to other authorities” such as juvenile or criminal court, a welfare agency, or probation or parole department (for those on probation or parole). This chapter also covers the Group B Arrestee Segment. The Arrestee Segment covers arrests for Group A offenses and there are corresponding Offense, Offender, and Victim segments for these incidents. Group B offenses, however, only have information about the arrest so incidents in this segment do not have any corresponding segments with it. Since Group B only has arrests without any associated incident, instead of the incident number variable like other segments have, this segment has an “arrest transaction incident number” which works the same as a normal incident number. Likewise, the Window Arrestee Segment is not associated with any other segments as the “window” part means that they are only partial reports. The Window Arrestee Segment has the same variables as the normal Arrestee Segment but also has 10 variables on each of the offenses committed (up to 10 offenses) during the incident. This is really to try to provide a bit of information that you would otherwise get from the other segments but do not since this is a window segment. For the rest of this chapter I will be using examples from the Arrestee Segment and not the Group B Arrestee (except for a table showing each Group B offense) or the Window Arrestee Segment. In addition to the variables detailed below this, segment has the traditional agency and incident identifiers: the ORI code, the agency state, the year of this data, and the incident number. It also has an “arrestee sequence number” which is an identifier for an arrestee in an incident since incidents can have multiple people arrested. This is just the number of each arrestee and to my knowledge is not associated with how involved the arrestee is. Being the 1st arrestee, for example, does not mean that individual played a greater role in the crime than the 2nd arrestee. 6.1 Crimes arrested for This segment tells us which offense the arrestee was arrested for. There are a couple of caveats with this data. First, there can be up to 10 crimes per incident but this segment only tells you the most serious offense (based on the agency’s decision of which is most serious). This can be solved partially by merging this segment with the Offense Segment and getting all of the offenses related to the incident. It is only partially solved because the crime the person is arrested for may not necessarily be the crime involved in the incident. This is because a person can be arrested for a certain crime (e.g. shoplifting) and then the police find out that there are also responsible for a more serious crime (e.g. aggravated assault). Their arrest crime is shoplifting and they will be associated with the incident for the aggravated assault. One interesting part of this segment is that while it is associated with Group A offenses, as someone may be arrested for a crime other than the crime in the incident, arrests can include Group B offenses. So the Group B Arrestee Segment can really be thought of as an arrest for a Group B offense where the arrestee is not associated with a previous Group A incident (other than ones that already led to an arrest since that incident would then be considered clear and the present arrest would not be associated with it). We will look first at the crimes people were arrested for in the main Arrestee Segment, which includes both Group A and Group B offenses as possible arrest crimes, and then at the Group B Arrestee Segment which only includes Group B offenses. 6.1.1 Arrestee Segment arrest crimes Table 6.1 shows the number and percent of arrests for all arrests associated with a Group A crime incident. Perhaps unsurprising, drug crimes are the most common arrest making up a quarter of all arrests (30% when including drug equipment crimes). Simple assault (assault without a weapon or without seriously injuring the victim) is the next most common at 19% of arrests, and aggravated assault is the 4th most common arrest crime at 6.3% of arrests. Theft, which NIBRS breaks into a number of subcategories of theft such as shoplifting and “all other larceny” is among the most common arrest crimes, making up ranks 3 and 6 of the top 6, as well as several other subcategories later on. The remaining crimes are all relatively rare, consisting of under 5% of arrests each. This is due to how the top crimes are broad categories (e.g. drug offenses ranges from simple possession to large scale sales but is all grouped into “drug/narcotic violations” here) while other crimes are specific (e.g. purse-snatching is a very specific form of theft). Table 6.1: The number and percent of arrests for Group A crimes for all arrests reported to NIBRS in 2022. Crime Category First Year # of Offenses % of Offenses Drug/Narcotic Offenses - Drug/Narcotic Violations 1991 651,593 22.44% Assault Offenses - Simple Assault 1991 625,914 21.55% Larceny/Theft Offenses - Shoplifting 1991 262,281 9.03% Assault Offenses - Aggravated Assault 1991 217,608 7.49% Drug/Narcotic Offenses - Drug Equipment Violations 1991 142,720 4.91% Weapon Law Violations - Weapon Law Violations 1991 123,203 4.24% Larceny/Theft Offenses - All Other Larceny 1991 121,677 4.19% Destruction/Damage/Vandalism of Property 1991 113,049 3.89% Assault Offenses - Intimidation 1991 85,136 2.93% Burglary/Breaking And Entering 1991 82,832 2.85% All Other Offenses 1991 71,084 2.45% Stolen Property Offenses (Receiving, Selling, Etc.) 1991 57,907 1.99% Motor Vehicle Theft 1991 52,882 1.82% Robbery 1991 38,048 1.31% Fraud Offenses - False Pretenses/Swindle/Confidence Game 1991 31,922 1.10% Larceny/Theft Offenses - Theft From Motor Vehicle 1991 23,673 0.82% Counterfeiting/Forgery 1991 20,107 0.69% Larceny/Theft Offenses - Theft From Building 1991 18,030 0.62% Driving Under The Influence 1991 16,410 0.57% Disorderly Conduct 1991 13,700 0.47% Kidnapping/Abduction 1991 12,869 0.44% Fraud Offenses - Impersonation 1991 11,225 0.39% Trespass of Real Property 1991 10,687 0.37% Sex Offenses - Fondling (Incident Liberties/Child Molest) 1991 10,298 0.35% Sex Offenses - Rape 1991 8,956 0.31% Murder/Nonnegligent Manslaughter 1991 8,284 0.29% Fraud Offenses - Credit Card/Atm Fraud 1991 7,281 0.25% Embezzlement 1991 6,689 0.23% Fraud Offenses - Identity Theft 2015 6,414 0.22% Arson 1991 6,338 0.22% Prostitution Offenses - Prostitution 1991 5,843 0.20% Larceny/Theft Offenses - Theft of Motor Vehicle Parts/Accessories 1991 5,621 0.19% Pornography/Obscene Material 1991 5,187 0.18% Liquor Law Violations 1991 4,142 0.14% Animal Cruelty 2015 3,904 0.13% Family Offenses, Nonviolent 1991 3,629 0.12% Larceny/Theft Offenses - Pocket-Picking 1991 3,323 0.11% Sex Offenses - Sodomy 1991 2,081 0.07% Prostitution Offenses - Assisting Or Promoting Prostitution 1991 1,559 0.05% Sex Offenses - Statutory Rape 1991 1,472 0.05% Prostitution Offenses - Purchasing Prostitution 2013 1,274 0.04% Curfew/Loitering/Vagrancy Violations 1991 1,191 0.04% Larceny/Theft Offenses - Purse-Snatching 1991 959 0.03% Sex Offenses - Sexual Assault With An Object 1991 912 0.03% Negligent Manslaughter 1991 858 0.03% Larceny/Theft Offenses - Theft From Coin-Operated Machine Or Device 1991 500 0.02% Extortion/Blackmail 1991 458 0.02% Human Trafficking - Commercial Sex Acts 2013 374 0.01% Gambling Offenses - Betting/Wagering 1991 365 0.01% Fraud Offenses - Wire Fraud 1991 360 0.01% Gambling Offenses - Operating/Promoting/Assisting Gambling 1991 279 0.01% Sex Offenses - Incest 1991 207 0.01% Bribery 1991 200 0.01% Fraud Offenses - Hacking/Computer Invasion 2015 125 0.00% Failure To Appear 2020 119 0.00% Fraud Offenses - Welfare Fraud 1991 112 0.00% Gambling Offenses - Gambling Equipment Violations 1991 102 0.00% Human Trafficking - Involuntary Servitude 2014 80 0.00% Runaway 1991 42 0.00% Fugitive Offenses - Flight To Avoid Prosecution 2021 24 0.00% Sex Offenses - Failure To Register As A Sex Offender 2021 11 0.00% Commerce Violations - Federal Liquor Offenses 2020 9 0.00% Gambling Offenses - Sports Tampering 1992 3 0.00% Immigration Violations - Illegal Entry Into The United States 2022 1 0.00% Weapon Law Violations - Explosives 2022 1 0.00% Fugitive Offenses - Harboring Escappee/Concealing From Arrest 2021 1 0.00% Perjury 2022 1 0.00% Federal Resource Violations 2021 1 0.00% Total 2,904,147 100% 6.1.2 Group B Segment arrest crimes Table 6.2 shows the number and percent of arrests for all arrests associated with a Group B crime incident. The offense categories overlap with Table 6.1 but these are for separate arrests, a single arrest cannot be in both segments. Unhelpfully, the most common Group B offense is “All other offenses” which means that it is a crime that is not covered in either the Group A or the Group B crime categories. However, this can also include Group A or Group B crimes if they are not completed. So an attempted or a conspiracy to commit a Group A or B crime can go in this category. At 57% of Group B arrests, this very vague category covers a huge amount of arrests. The next most common Group B arrest is driving under the influence of drugs or alcohol, and this occurred in 18.4% - or 352k times - of arrests. Trespassing makes up 5.7% of arrests and this is unlawfully entering someone’s property, including a building. The difference between this and burglary is that this is entering without any intent to commit theft or a felony. Disorderly conduct is a broad category ranging from indecent exposure (which should be its own sex offense but is not for some reason) to “profanity” and noise violations, and it makes up 6.2% of arrests. So be cautious with this offense as it ranges from very minor to quite serious and there is no distinguishing what actually happened. Drunkenness and liquor law violations make up 6% and 3.6% of arrests, respectively. The difference is that drunkenness is when someone is seriously drunk in public (to the point where they cannot control their own body) and liquor law violations are about illegal making or selling of liquor. So basically bootlegging, selling alcohol without a license (or to people not allowed to drink, like minors), or avoiding paying tax on alcohol sales. “Family Offenses, Nonviolent” is also a rather vague category and includes “nonviolent abuse” (which I guess means emotional abusive) as well as neglecting the child in a few different ways like not paying alimony and deserting the child. Since these are arrests, the actions have to reach the level of criminal behavior, simply being an awful parent (or even leaving the child, as long as they have another adult to watch them) is not this offense. Runaways is an offense that only applies to people under age 18. The remaining categories are all rare and none are more than 1% of arrests. Table 6.2: The number and percent of arrests for Group B crimes for all arrests reported to NIBRS in 2022. Crime Category First Year # of Offenses % of Offenses All Other Offenses 1991 1,470,699 58.45% Driving Under The Influence 1991 565,429 22.47% Trespass of Real Property 1991 177,586 7.06% Disorderly Conduct 1991 176,937 7.03% Liquor Law Violations 1991 72,044 2.86% Family Offenses, Nonviolent 1991 33,545 1.33% Curfew/Loitering/Vagrancy Violations 1991 16,266 0.65% Failure To Appear 2020 1,816 0.07% Runaway 1991 1,629 0.06% Federal Resource Violations 2021 15 0.00% Perjury 2022 1 0.00% Total 2,515,967 100% 6.2 Arrest date For each arrest we know the exact date of the arrest. As with the incident date, there is evidence that when agencies do not know the exact arrest date, they put down the first of the month. However, this is far less of a problem than with the incident date, likely because since the agency is doing the arresting they know exactly when they do it. Instead of looking at arrests by day of the month, we will use both the arrest date and the incident date to look at how long it takes for crimes to get solved. Figure 6.1 shows how long it takes for arrests to be made. The shortest time is zero days which means the arrest and the incident happened on the same day and the longest is 461 days after the incident. About 76.5% of arrests happen on the same day as the incident while 6.6% happen on the next day. 1.4% happen the following day and 1% on the day after this. This trend of a lower probability of the case being solved as the time from the incident increases continues throughout the figure. Including dates up to 461 days is a bit ridiculous since it is impossible to see trends among the early dates other than zero days, but it is a good demonstration of how massively concentrated arrests are that occur on the same day of the incident. The lesson here is that if an arrest is not made on the day of the incident (such as at the scene of the crime), it is very unlikely that’ll it’ll be made at all (and most crimes never lead to an arrest). Figure 6.1: The distribution of the number of days from the incident to the arrest date. In 2022 the maximum days from incident to arrest was 461 days. Zero days means that the arrest occurred on the same day as the incident. Figure 6.2 groups the larger number of days together to make it easier to see trends early after the incident. Here we can see much better how the percent of arrests move quickly downwards after zero days. Figure 6.2: The number of days from the incident to the arrest date. Values over 10 days are grouped to better see the distribution for arrests that took fewer than 10 days. Zero days means that the arrest occurred on the same day as the incident. 6.3 Weapons In the Offense Segment we get info on what weapon (if any) was used during the crime. Here, we see what weapon the person arrested was carrying when they were arrested. There is probably a very large overlap here, especially given that the vast majority of arrests happen on the same day as the offense, so probably happen at the scene of the crime (and we will see exactly which ones do happen there later on in this chapter). Compared to the weapons covered in the Offense Segment - see Section ?? for more - the weapons here are only a narrow subset, and cover mostly firearms. This is partly because the most common “weapon” in the Offense Segment is that the offender used their body as a weapon (e.g. punching, kicking) and everyone arrested has a body so it does not make sense to count that as a weapon. Each arrestee can carry up to two weapons, but we will only look at the first weapon for the below graphs. Please note that this is weapons found on the arrestee, and does not mean that they used the weapon during the arrest. Figure ?? shows the breakdown in the weapon carried by the arrestee during the arrest. In 94% of arrests, the arrestee was not carrying any weapon. Since this graph shows arrests for all crimes, it makes a good deal of sense. Most crimes are non-violent so we would expect those people to not carry a weapon. Since so few arrestees have weapons, we will look at the breakdown among those who were carrying a weapon in Figure 6.4. To see the weapons carried when the arrestee had a weapon, Figure 6.4 shows the breakdown in which weapon they carried. About 43.8% of people arrested who had a weapon were carrying a handgun followed by 30% with some kind of “lethal cutting instrument” like a knife. While rifles, and especially “assault rifles”, are what people (and especially politicians and the media) focus on when talking about violent crime, handguns are actually the most common gun to be used in a crime so it makes sense that handguns are the most frequently found weapon. “Firearm (type not stated)” basically means that the type of firearm used is unknown so can belong in one of the firearm categories and makes up 9% of weapons. Blunt instruments (including bats, clubs, and brass knuckles) follow at 6.9% of weapons. And the remaining weapons included are “other firearm” (so any other than ones specified) at 5.8%, rifle at 2.3%, and shotgun at 2%. Figure 6.3: The share of murder and nonnegligent manslaughter arrestees by weapon carried at arrest, 1991-2022. Figure 6.4: The distribution of weapon usage for all arrestees in 2022 who were arrested with a weapon (i.e. excludes unarmed arrestees). 6.4 Automatic weapons This variable tells you if the weapon the arrestee was carrying was a gun whether that gun was fully automatic. To be clear, this means that when you pull the trigger once the gun will fire multiple bullets. Semi-automatic firearms are not automatic firearms. The Offense Segment also has a variable indicating if the offender used an automatic weapon but there they did not necessarily recover the gun so it is much less reliable than in this segment where the police have the gun and are able to test it.28 The percent of guns that are fully automatic are fairly similar between the weapons seized at arrest, as shown in Figure 6.5 and those used in the offense as shown in Figure ?? in Chapter ??. Figure 6.5 shows that about 5.6% of rifles seized by police during an arrest were fully automatic. About 4.9% of handguns are automatic while “firearm (type not stated) are automatic in 4.3% of cases. Shotguns and”other firearm” category are the least likely to be automatic at about 2.5% and 1.1% of weapons, respectively. Figure 6.5: The percent of firearms the arrestee was carrying that were fully automatic, for arrestees in 2022. 6.5 Type of arrest While arrests are sometimes talked about as if they are a homogeneous group (likely in big part because UCR data does not differentiate types of arrests), NIBRS data breaks them down into three different types of arrests. Figure ?? shows the distribution in the type of arrest for all arrestees in the 2022 NIBRS data. The most common type is “On-View” which means that the person is arrested at the scene by an officer. For example, if police respond to a bank robbery and nab the robbers as they run out of the bank, this is an on-view arrest. On-view arrests make up the majority - 50.9% - of arrests. The next type of arrest is a “warrant/previous incident report” and this makes up 26.8% of arrests. In these cases the police had an arrest warrant and found the person and arrested them based on that warrant. This also includes when a person is arrested and found to have been involved in previous incidents. Then these previous incidents would be considered cleared from this single arrest. In these cases NIBRS has an indicator variable, called the “multiple arrestee indicator” to tell us that this individual is responsible for multiple incidents cleared so we avoid counting them twice (as their demographics will be the same each time). In this variable it says “count arrestee” if this is their only arrest or if this is the first arrest that is counted in cases where multiple incidents are cleared by the arrest, and “multiple” otherwise. Finally, people can get a “summoned/cited” arrest which is not really an arrest at all. This is when they are given a subpoena that says that must go to court to be tried for a crime, but they are not formally arrested and taken into police custody (i.e. they are never handcuffed, taken to a police precinct or to jail). About 22.2% of arrests are this form of arrest. THEYRE Figure 6.6: Annual arrest type for all arrestees, 1991-2022. Previous Incident Report includes cases where an individual was arrested for a separate crime and are then reported as also arrested for this incident. Figure 6.7: Annual arrest type for DUI arrestees, 1991-2022. 6.6 Disposition for juvenile arrestees For juvenile arrestees - those under age 18 at the time of the arrest (and, by definition they would also be under age 18 during the incident) - there is some information about the outcome of the arrest.29 There are two possible outcomes (which NIBRS calls “dispositions”): being referred to other (that is, other than the arresting agency) authorities or handled within the arresting agency. Figure 6.8 shows this breakdown and being referred to other authorities is the most common outcome at 72.6% of juvenile arrests. This is a very broad category and the “other authorities” can range from juvenile or adult court (so the police recommend that they be prosecuted) to welfare agencies and being sent to other police agencies (such as if they committed a crime elsewhere and are being extradited). The other category, being handled within the department, means that the police release the juvenile without any formal action taken (but they may give the juvenile a warning). In these cases the juvenile is released to an adult (including but not limited to family members or guardians) and the case is essentially dropped. In about 0.001% of juvenile arrests the disposition is unknown.30 Figure 6.8: For juvenile arrestees (under age 18), the distribution of case outcomes. 6.7 Demographics This segment provides several variables related to who the arrestee is. Age, race, and sex overlap with the Offender Segment but this segment also adds ethnicity and whether they live in the jurisdiction of the agency (i.e. the city the police patrols) they were arrested by. 6.7.1 Residence status The first variable we will look at is the residence status for the arrestee. This is residence in the jurisdiction that arrested them and it has nothing to do with residence status or citizenship status in the United States. People tend to commit crimes (and are the victims of crimes) very close to where they live, so this provides some evidence for that. We do not know where the arrestee lives, but know if they live in the jurisdiction or not. This is useful because some areas (e.g. Las Vegas, Washington DC, urban city centers where people commute to work) likely have a lot more people moving into those areas during the day but who live else compared to places like rural towns or suburbs. So it is helpful to be able to distinguish locals arrested with people who may be tourists or come into town just to commit the crime.31 One thing to be cautious about is that residence status may be an imprecise measure of where someone actually lives. How it is defined may differ by agency which could affect comparability across agencies. For example, if it is defined as official residence (such as address on a driver’s license) that may be incorrect for a sizable share of the population (e.g. many college students live far from where their driver’s license address is).32 If residence status is based on asking the arrestee, they may of course lie to the police. There’s also the question of how to label people who are truly transient such as homeless people who may move from city to city. The FBI defines residence as their legal permanent address though it is unclear how that is handled for people without this info and when people live permanently in a different spot than their legal address. Figure ?? shows the percent of arrestees in 2022 who were residents or not (or whose status was unknown) of the jurisdiction that arrested them. Most people were arrested near where they live, with 56.9% of arrestees being residents, compared with 23.3% not being residents. However, about one-fifth of arrestees had an unknown residence status so the percents of resident and non-resident may change dramatically if we did not have any unknowns. Figure 6.9: Annual resident status (i.e. if they live in the arresting jurisdiction) of arrestees, 1991-2022. 6.7.2 Age This variable is the age at the arrest, which may be different than age during the crime. As in the Offender Segment we are given the exact age (in years) but agencies can input a range of possible ages with the FBI giving us the average of this range (rounding down, not to the nearest integer) in the data. In Figure ?? in the Offense Segment, this can be seen in the sudden spikes in the percent of offenders of a certain age and that some of the most common ages are divisible by five (e.g. 20, 25, 30). There are also far fewer unknown ages in this data with only 0.1% of arrestees having a missing age. This is reasonable as a person arrested is present for the police to learn their age from something like a driver’s license or past criminal records, or estimate the age by looking at the arrestee. Like in the Offender Segment, age as a specific year is cutoff at 98 with all older ages grouped simply as “over 98 years old”. Figure 6.10 shows the percent of arrestees at every age available. Relative to Figure ??, this graph is far smoother, indicating that there was less estimating ages and more knowing the actual age. While the trend is the same for both of these graphs, the arrestee data does not have any odd spikes with certain ages. Age we see that the percent of people arrested increases as they age, peaking in the early twenties before declining and then peaking age even higher in the late 20s. After this, there is a long steady decline as the arrestee ages. Figure 6.10: The age of all arrestees reported in the 2022 NIBRS data. Figure 6.11: The mean and median age of arrestees, 1991-2022. Figure 6.12: The percent of arrestee’s age that is unknown, 1991-2022. 6.7.3 Sex We also know the sex of the arrestee. The only options for this variable are male and female and there is never missing data so the police always choose one of these two choices. There is no option for transgender or any other identity. Figure ?? shows the distribution of arrestees by sex. The vast majority, 70.5% of arrestees are male and the remaining 29.5% are female. This is a higher rate of female arrestees than you might expect - past research has found that crime is largely a male-phenomenon, even greater than found here (though “crime” in most criminology research is only murder or violent index crimes) - and that is because there are differences in sex involvement by type of crime. For rape, as an example, 97.8% of arrestees in 2022 were male. Shoplifting was an even 50% split between female and male arrestees. Figure 6.13: The share of arrestees by sex, 1991-2022. 6.7.4 Race For each arrestee we know their race, with possible races being White, Black, American Indian/Alaskan Native, Asian, and Native Hawaiian/Pacific Islander. Unlike sex, the police can say that the race is unknown.33 As each arrestee is visible to the police, and can self-report race or provide official records (e.g. criminal history or jail admission data) which may say their race, there is far less uncertainty for arrestee race than offender race where 38.5% of the data is missing. As with any measure of race there is still some degree of uncertainty since people’s race are not always obvious and may not fit tidily into each of the mutually exclusive groups available in NIBRS data (e.g. there is no option for mixed race). Figure ?? shows the breakdown for the races of each arrestee. White arrestees are most common at 64.2% of arrestees, following by Black arrestees at 29.8%. Only 3.1% of arrestees have an unknown race. The remaining small share of arrestees is made up of American Indian/Alaskan Native at 1.6%, Asian at 1%, and Native Hawaiian/Pacific Islander at 0.3% of arrestees. Figure 6.14: The share of arrestees by race, 1991-2022. 6.7.5 Ethnicity Finally, there is data on the race of the arrestee so we know if they are Hispanic or not. Ethnicity is so poorly used in the UCR data (e.g. UCR stopped collecting it for arrests for most years available and most agencies still do not report it) that I recommended in the UCR book against ever using it. For NIBRS, there is far less data missing so it is not as much of a problem to use ethnicity as it is with UCR data. The issue remains as to what agencies are actually reporting this data or in which scenarios this variable is reported or not even in agencies that generally do report it. Ethnicity is an optional variable so agencies are never required to report it. This means that there is a greater chance that it’ll be used only in non-random situations (which would make the data biased in some unknown way). There’s also the question of reliability of the ethnicity data. Someone being Hispanic or not is likely just what the arrestees calls themselves or what the arresting officer perceives them to be. Both are important ways of measuring ethnicity but get at different things. Perception is more important for studies of bias, self-identification for differences among groups of people such as arrest rates by ethnicity. And the subjectivity of who is classified as Hispanic means that this measurement may differ by agency and by officer, making it imprecise. Figure ?? shows the breakdown in arrests by arrestee ethnicity. Most arrestees - 72.5% - are not Hispanic. Only 10% are reported to be Hispanic but an even higher percent of arrestees - 16.8% - have an unknown ethnicity so the true percent or Hispanic or non-Hispanic arrestee may be different than shown here. Figure 6.15: The share of arrestees by ethnicity, 1991-2022. It is not clear whether they actually test it or simply go by the design of the gun, such as if the model allows for fully automatic firing.↩︎ There are a few people older than 18 with this variable but it is so rare that I think that they are just incorrectly inputted ages.↩︎ A juvenile can potentially get multiple dispositions, such as if they are initially released but later the police recommend them for prosecution. It is not clear which outcome is recorded in these cases. In UCR data, however, only the initial disposition is recorded so that is likely how it also is in NIBRS.↩︎ In a ride-along I was on, a woman who lived over an hour from the city I was in was caught shoplifting clothes.↩︎ One another ride-along, a man from Florida was arrested for stealing from a local store (in California).↩︎ I have been told that measuring race at all is itself racist so should never been done, even in research. This from a group of people who also said they have no need to actually evaluate police racial bias properly (i.e. using a regression with control variables) since they already knew the answer. Luckily I am no longer a postdoc.↩︎ "],["property.html", "Chapter 7 Property Segment 7.1 Type of property loss 7.2 Description of property 7.3 Value of stolen property 7.4 Date property was recovered 7.5 Drugs", " Chapter 7 Property Segment The Property Segment provides a bit more info than would be expected from the name. For each item involved in the crime it tells you what category that items falls into, with 68 total categories (including “other”) ranging from explosives and pets to money and alcohol. It also tells you the estimated value of that item. This data covers more than just items stolen during a crime. For each item it tells you what happened to that item such as if it was stolen, damaged, seized by police (such as illegal items like drugs), recovered (from being stolen) by police, or burned during an arson. For drug offenses it includes the drugs seized by police. For these offenses, the data tells us the type of drug, with 16 different drug categories ranging from specific ones like marijuana or heroin to broader categories such as “other narcotics”. There can be up to three different drugs included in this data - if the person has more than three types of drugs seized then the third drug category will simply indicate that there are more than three drugs, so we learn what the first two drugs are but not the third or greater drugs are in these cases. For each drug we also know exactly how much was seized with one variable saying the amount the police found and another saying the units we should we reading that amount as (e.g. pills, grams, plants). In addition to the variables detailed below, this segment has the traditional agency and incident identifiers: the ORI code, the agency state, the year of this data, the incident number, and the incident date. The Window Property Segment has the same variables as the normal Property Segment but also has 10 variables on each of the offenses committed (up to 10 offenses) during the incident. This is really to try to provide a bit of information that you’d otherwise get from the other segments but don’t since this is a window segment. For the rest of this chapter I’ll be using examples from the Property Segment and not the Window Property. 7.1 Type of property loss This segment contains information on all property involved in the incident, other than the weapon used by the offender. Property can be involved in multiple ways - such as being stolen, destroyed, or that the stolen property is recovered by the police - so NIBRS breaks this info into seven different categories (eight when including “unknown” type). Figure 7.1 shows each of these categories as how often they occur. The most common category, at 60.7% of the rows in this segment, is when the item is taken from the victim by the offender. This includes when the offender stole the item, took it by force in a robbery, tricked the victim (e.g. offender committed fraud), and any other way to illegal get the item (e.g. extortion, ransom, bribery). Though it includes all these types of ways to illegally take the item, it is often just called “stolen” property and I will refer to it as such in this chapter just for simplicity of writing. The next most common group, at 13.3%, is when the item was seized by the police. This excludes items that were stolen and that the police recovered. It includes all types of property that the offender illegally had but is primary for drug crimes where the drug or drug equipment was seized. This segment also includes property damaged, destroyed, or vandalized by the offender and this type makes up 11.2% of the data. Following this, about 9% of rows are for recovered property which is when one of the previously stolen items is recovered by the police. Next is “none” which only means that no property was stolen or damaged but that it could have been. For example, if someone tries to grab a person’s purse but fails, that would be considered “none” since the purse was not actually taken. The remaining types are when the item was a counterfeit/forgery, at 1.2%, was burned (such as during an arson) at 0.2%, or when the type of loss is unknown at 0.2% Figure 7.1: The type of loss or if the item is recovered. Figure 7.2: The annual number of agencies reporting data in the NIBRS Property Segment, 1991-2022. 7.2 Description of property There are 68 different categories of properties (including the catch-all “other” category for anything not explicitly in a different category) that NIBRS covers. This also includes “identity - intangible” which means that someone stole the victim’s identity. Identity is not property since it is not a physical thing but is nonetheless included here (items related to one’s identity such as social security cards are included in a different category called “identity documents”). Each of these categories can appear in any of the seven different types of property loss discussed above. For simplicity of writing, and because Table 7.1 covers items stolen, I will be talking below about items lost by being stolen, though everything does apply to other types of loss too. The property included in NIBRS can move from very broad categories like “merchandise” to more specific items such as livestock, aircraft, and pets. The property categories are mutually exclusive so a single item cannon be counted in different categories. If, for example, laptop is stolen that could potentially go in “computer hardware/software” or “office-type equipment” but it would not be in both, it would only be recorded in one of the two. In cases where multiple items of the same type are stolen - such as someone stealing multiple laptops in a single crime - we do not actually know how many items were stolen. Just that at least one item of that category was stolen in the incident. We then know the total estimated value of the items stolen in that incident which we can use to estimate the number of items stolen (as long as the know the average value per item in that category) though this would be a very imprecise measure. The exception to this is for stolen vehicles where we know exactly how many were stolen and how many were recovered. Table 7.1 shows each of the 68 different types of property in NIBRS and shows the number of incidents where they were stolen for all incidents in the 2022 NIBRS data. Multiple different categories of property can be stolen in a single incident. The most common type of property stolen, at 14.3% of all property is “other” which is not really that helpful to us. We know it is not one of the other 67 categories but not exactly what was stolen. Next, are money at 11.8% and then purses/handbags/wallets at 5.6%. This makes a lot of sense. People steal things for financial gain and the easiest way to get that gain is stealing money directly (or wallets and purses which often have money inside). Stealing property that you then have to sell or trade to get what you want (money or other property) is a lot harder, which is likely why it is less common. There are about 4.8 million rows in the Property Segment where the item was stolen (the other 3.1 million rows are about property that was seized by police, recovered, damaged, or one of the other types of property loss detailed in Section 7.1). As such, even items stolen very rarely can occur thousands or tens of thousands of times. For example, pets were the property stolen about 0.14% of the time and that tiny percent still accounts of 6,821 incidents with a pet stolen. Given the huge number of rows in this data - and as more agencies report to NIBRS this will grow quickly - it can be possible to study specific types of property, such as pets stolen, that would not be possible with a more narrow dataset (both in terms of how specific the items they include are, and the size of the data). Table 7.1: The number and percent of property stolen (including forcibly taken such as during a robbery) in a crime, for all offenses in 2022. Each incident can have multiple items stolen. Property First Year # of Property Stolen % of Property Stolen Other 1991 1,149,292 15.09% Money 1991 824,234 10.82% Automobiles 1991 570,623 7.49% Vehicle Parts/Accessories 1991 535,780 7.03% Purses/Handbags/Wallets 1991 377,761 4.96% Clothes/Furs 1991 348,984 4.58% Credit/Debit Cards 1991 313,888 4.12% Tools - Power/Hand 1991 309,668 4.07% Merchandise 1991 289,078 3.79% Consumable Goods 1991 253,272 3.32% Identity Documents 2009 252,837 3.32% Portible Electronic Communications 2009 244,450 3.21% Computer Hardware/Software 1991 201,441 2.64% Household Goods 1991 200,120 2.63% Firearms 1991 170,635 2.24% Identity - Intangible 2009 162,824 2.14% Radios/Tvs/Vcrs 1991 144,002 1.89% Jewelry/Precious Metals 1991 141,635 1.86% Bicycles 1991 123,395 1.62% Documents - Personal Or Business 2009 94,572 1.24% Trucks 1991 93,364 1.23% Alcohol 1991 75,575 0.99% Other Motor Vehicles 1991 59,593 0.78% Negotiable Instruments 1991 54,250 0.71% Office-Type Equipment 1991 53,825 0.71% Lawn/Yard/Garden Equipment 2009 47,867 0.63% Trailers 2009 45,526 0.60% Building Materials 2009 43,258 0.57% Drugs/Narcotics 1991 42,540 0.56% Recreational/Sports Equipment 2009 40,414 0.53% Nonnegotiable Instruments 1991 31,060 0.41% Fuel 2009 30,233 0.40% Camping/Hunting/Fishing Equipment/Supplies 2009 28,371 0.37% Photographic/Optical Equipment 2009 27,502 0.36% Heavy Construction/Industrial Equipment 1991 24,878 0.33% Metals, Non-Precious 2009 18,589 0.24% Collections/Collectibles 2009 18,126 0.24% Firearm Accessories 2009 18,115 0.24% Weapons - Other 2009 17,843 0.23% Pending Inventory (Of Property) 1991 17,818 0.23% Recordings - Audio/Visual 1991 17,146 0.23% Medical/Medical Lab Equipment 2009 13,153 0.17% Musical Instruments 2009 12,275 0.16% Recreational Vehicles 1991 11,395 0.15% Pets 2009 11,368 0.15% Farm Equipment 1991 8,892 0.12% Artistic Supplies/Accessories 2009 6,265 0.08% Chemicals 2009 5,326 0.07% Explosives 2010 4,542 0.06% Gambling Equipment 1991 4,482 0.06% Watercraft 1991 4,064 0.05% Drug/Narcotic Equipment 1991 4,000 0.05% Watercraft Equipment/Parts/Accessories 2009 3,186 0.04% Law Enforcement Equipment 2009 2,773 0.04% Livestock 1991 2,242 0.03% Logging Equipment 2009 1,470 0.02% Structures - Other 1991 1,311 0.02% Crops 2010 1,199 0.02% Buses 1991 1,121 0.01% Aircraft Parts/Accessories 2009 1,053 0.01% Structures - Single Occupancy Dwellings 1991 624 0.01% Structures - Storage 1991 546 0.01% Aircraft 1991 491 0.01% Structures - Commercial/Business 1991 438 0.01% Structures - Other Dwellings 1991 348 0.00% Special Category 1991 281 0.00% Structures - Industrial Manufacturing 1991 224 0.00% Structures - Public/Community 1991 207 0.00% Total 7,617,660 100% As seen in Table 7.2, all of the 68 different types of properties available in NIBRS can and, in 2022, were seized by police during an incident at least once. This includes atypical property like building material, musical instruments, and pets (and leads me to think that at least some of this is incorrectly labeled as property seized by the police). The vast majority of property seized by police, however, is drugs. 63.2% of the property seized were drugs themselves while 27.3% were equipment related to the drugs. The remaining items were mostly “other” (i.e. anything not explicitly categorized here) at 2.6%, money at 2%, firearms at 0.9%, and then a bunch of very rarely seized items. Table 7.2: The number and percent of property seized by police (excludes recovering property that was stolen, for all offenses in 2022. Each incident can have multiple items seized. Property # of Property Seized % of Property Seized Drugs/Narcotics 916,616 63.68% Drug/Narcotic Equipment 391,785 27.22% Other 37,857 2.63% Money 21,762 1.51% Firearms 17,471 1.21% Portible Electronic Communications 7,798 0.54% Firearm Accessories 4,740 0.33% Automobiles 4,037 0.28% Documents - Personal Or Business 3,580 0.25% Identity Documents 3,477 0.24% Purses/Handbags/Wallets 2,852 0.20% Weapons - Other 2,699 0.19% Credit/Debit Cards 1,981 0.14% Consumable Goods 1,954 0.14% Office-Type Equipment 1,899 0.13% Vehicle Parts/Accessories 1,712 0.12% Negotiable Instruments 1,521 0.11% Recordings - Audio/Visual 1,505 0.10% Computer Hardware/Software 1,494 0.10% Alcohol 1,433 0.10% Clothes/Furs 1,350 0.09% Household Goods 1,300 0.09% Explosives 1,267 0.09% Tools - Power/Hand 1,131 0.08% Nonnegotiable Instruments 1,106 0.08% Merchandise 525 0.04% Gambling Equipment 503 0.03% Radios/Tvs/Vcrs 467 0.03% Heavy Construction/Industrial Equipment 407 0.03% Other Motor Vehicles 363 0.03% Jewelry/Precious Metals 358 0.02% Medical/Medical Lab Equipment 329 0.02% Trucks 274 0.02% Photographic/Optical Equipment 231 0.02% Identity - Intangible 217 0.02% Chemicals 181 0.01% Bicycles 160 0.01% Pending Inventory (Of Property) 142 0.01% Camping/Hunting/Fishing Equipment/Supplies 96 0.01% Collections/Collectibles 94 0.01% Recreational/Sports Equipment 81 0.01% Law Enforcement Equipment 68 0.00% Metals, Non-Precious 54 0.00% Farm Equipment 54 0.00% Artistic Supplies/Accessories 53 0.00% Building Materials 49 0.00% Trailers 45 0.00% Structures - Other 40 0.00% Recreational Vehicles 34 0.00% Crops 29 0.00% Lawn/Yard/Garden Equipment 29 0.00% Fuel 23 0.00% Structures - Storage 20 0.00% Aircraft 17 0.00% Musical Instruments 16 0.00% Pets 11 0.00% Aircraft Parts/Accessories 10 0.00% Livestock 9 0.00% Structures - Public/Community 8 0.00% Structures - Commercial/Business 7 0.00% Buses 6 0.00% Watercraft Equipment/Parts/Accessories 6 0.00% Structures - Industrial Manufacturing 4 0.00% Structures - Single Occupancy Dwellings 4 0.00% Logging Equipment 4 0.00% Watercraft 3 0.00% Structures - Other Dwellings 3 0.00% Special Category 2 0.00% Total 1,439,363 100% 7.3 Value of stolen property For all types of property loss other than the property being seized by the police (and when the type is “unknown”) there is data on the estimated value of that property. This is estimates by the police but is supposed to be the current value of the item (e.g. a stolen car is what it currently sells for, not what the buyer bought it for) and is the cost it will take for the victim to replace the item. To be a bit more specific, this variable is the sum of items stolen in this category. For example, if someone burglarizes a house and steals three rings from the victim, this would not count as three thefts of a ring. It would be recorded as loss of jewelry and the value would be the total value of all three rings. The police can take the victim’s estimation into consideration but are not supposed to immediately accept it as victims may exaggerate values (especially for insurance purposes). When items are recovered, the police put in the value at the time of recovery which may be substantially different than at the time of the loss if the item is damaged or destroyed. We can use this variable to look at the value of items lost by the victim. Figure 7.3 looks at the value per item stolen in a crime (if incidents have multiple items stolen, this counts them all separately). This includes items lost by theft, robbery, and burglary so is part of the “Stolen/Robbed/Etc.” category of types of property loss. It excludes items damaged or destroyed or burned (there is info about the value of property in these incidents but these are not shown in the figure). To make this graph a bit simpler I have rounded all values to the nearest $100 so items valued at $0 mean that they are worth between $1 and $50. I have also capped the largest value to $1,000,000 and set the x-axis on the log scale since the data skews very much to the right. The average item lost was worth $3,217 and the median was worth $150. Most items lost were relative cheap with 34.9% worth under $50 and 13.5% worth between $51 and $149. There is a sharp decline in the frequency of property stolen as the value increases. So while some extremely valuable items are lost during crimes, they are much less common than relatively low-value items. Figure 7.3: The distribution of the value of property stolen. Values are capped at 1,000,000 and each value is rounded to the nearest 100. The x-axis is set on the log scale as this distribution is hugely right skewed. Since multiple items can be stolen in a single incident, to better understand the cost of crime for the victim we probably want to add up all of the property to the incident-level. Figure 7.3 does this and shows the cost of property stolen per incident. The trend is nearly identical to Figure 7.2 but the value is a bit higher than before. Now fewer than a quarter of incidents result in a loss of &lt;$50 and the average cost per incident is $4,731 (median = $300). Figure 7.4: The incident-level distribution of the value of property stolen. As values are aggregated to the incident-level, these are higher than the above graph which shows each item individually. Values are capped at 1,000,000 and each value is rounded to the nearest 100. The x-axis is set on the log scale as this distribution is hugely right skewed. Table 7.3: Annual mean, median, and maximum values (in dollars) of property stolen and recovered, 1991-2022. Year Mean Value Stolen Median Value Stolen Max Value Stolen Mean Value Recovered Median Value Recovered Max Value Recovered 1,991 1,051 150 15,016,780 1,727 125 12,000,000 1,992 3,242 148 580,000,682 1,414 100 1,790,000 1,993 14,701 129 900,000,000 23,215 95 900,000,000 1,994 41,258 129 999,999,999 64,414 85 999,999,999 1,995 2,074 134 450,000,000 6,196 81 450,000,000 1,996 2,560 147 800,000,085 1,750 85 3,500,000 1,997 1,640 135 450,000,000 1,830 92 1,300,000 1,998 1,898 146 480,000,000 1,978 100 1,000,000 1,999 1,791 150 245,000,120 2,621 100 100,000,000 2,000 1,252 150 100,001,280 2,379 124 838,466 2,001 8,128 143 890,000,000 4,397 125 500,000,001 2,002 1,742 150 300,000,000 3,438 125 300,000,000 2,003 1,814 140 400,000,000 2,384 119 7,501,000 2,004 1,833 130 939,000,000 4,918 120 939,000,000 2,005 2,164 150 575,152,425 3,687 150 298,000,001 2,006 1,315 150 100,003,300 2,656 174 5,726,400 2,007 1,850 150 950,000,000 2,650 150 100,000,000 2,008 3,031 150 999,999,999 4,434 120 999,999,999 2,009 1,179 155 100,000,000 1,719 100 5,000,000 2,010 1,150 170 10,000,000 1,606 100 2,199,999 2,011 1,462 180 999,999,999 1,516 100 2,001,850 2,012 1,222 175 100,000,000 1,554 103 18,805,960 2,013 1,792 168 950,000,000 1,788 108 100,000,020 2,014 1,271 150 100,000,350 1,667 109 1,800,000 2,015 1,257 150 20,000,000 1,867 115 2,500,000 2,016 2,063 150 999,999,999 2,199 150 4,000,000 2,017 1,561 150 361,000,000 2,520 160 5,000,000 2,018 2,009 150 313,600,000 2,911 172 24,000,000 2,019 3,217 150 999,999,999 5,543 168 999,999,999 2,020 229,834 170 999,999,999 129,068 223 999,999,999 2,021 284,937 207 999,999,999 250,610 400 999,999,999 2,022 72,129 250 999,999,999 81,535 462 999,999,999 Figure 7.5: Annual percent of the value of all property stolen that is made up of the value that is the maximum dollar amount reported in that year, 1991-2022. 7.4 Date property was recovered This segment tells us both when the incident happened and, for stolen property, when the item was recovered. We can use this to look at how long it generally takes for property to be recovered (though most property stolen is never recovered). Figure 7.6 shows the number of days it takes for property to be recovered. Though this data gives us the exact date, allowing for the precise number of days from property loss to recovery, this graph groups days greater than nine days to simplify the graph. The maximum number of days in the 2022 NIBRS data is 450 days so showing all days would be a rather unhelpful graph. The majority - 60.8% - of property lost is recovered on the same day, which is shown as zero days. We saw in Figure 6.1 that the vast majority of arrests happen on the same day as the incident so it makes sense the most property would too.34 A smaller and smaller share of property is recovered as the number of days from the incident increase, a trend also found in the time to arrest graph. The lesson here seems to be that if you are a victim of a crime and had something taken, unless it is recovered very quickly it is unlikely to be recovered at all. Figure 7.6: The distribution of the number of days from the incident to the property recovered date. In 2022 the maximum days from incident to arrest was 456 days. Zero days means that the arrest occurred on the same day as the incident. 7.5 Drugs This segment also provides information about drugs seized by the police. This also includes cases where the police would have seized the drugs if the offender did not destroy it somehow, such as flushing it down the toilet. For each drug seized there is information on what type of drug it was and how much of the drug was seized. There can be up to three drugs seized in an incident and data is available for each type of drug. The exception, however, is when there are more than three drugs seized in an incident. In that case, info on the third drug just says that there were more than three drugs involved. So you would have info on the first two drugs but none on the third through however many (and it does not say how many) drugs. For the below table and figure I only look at the first drug seized, so the numbers shown are undercounts. Figure 7.7: The annual number of drug seizures reported, 1991-2022. The ordering of drugs when there are multiple drugs is based on how much drugs were recovered and the seriousness of the drugs. For example, heroin is probably considered more serious than marijuana, but overall ranking of drugs is a subjective assessment depending on the department. Is heroin more serious than meth? That decision likely varies by the agency and their situation in regard to what drugs they often seize. 7.5.1 Suspected drug type The drugs in NIBRS are the “suspected drug types” which means that they are what the police believe the drugs to be, even if there is not definitive proof (such as a crime lab testing for what type of drug it is) that the drug is what they say it is. There are 15 possible drug types in NIBRS (16 when including “unknown drug type”) and each is shown in Table 7.4 along with how often they occur in the data. Some of these drug types are specific enough to only include a single drug while others are groups of drug types, such as hallucinogens or stimulants (and they include all of these types other than specifically stated drugs). Amphetamines/Methamphetamines Barbiturates Cocaine (All Forms Except Crack) Crack Cocaine Hashish Heroin LSD Marijuana Morphine Opium Other Depressants: Glutethimide Or Doriden, Methaqualone Or Quaalude, Pentazocine Or Talwin, Etc. Other Drugs: Antidepressants (Elavil, Triavil, Tofranil, Etc.), Aromatic Hydrocarbons, Propoxyphene Or Darvon, Tranquilizers (Chlordiazepoxide Or Librium, Diazepam Or Valium, Etc.), Etc. Other Hallucinogrens: BMDA (White Acid), DMT, MDA, MDMA, Mescaline Or Peyote, Psilocybin, STP, Etc. Other Narcotics: Codeine, Demerol, Dihydromorphinone Or Dilaudid, Hydrocodone Or Percodan, Methadone, Etc. Other Stimulants: Adipex, Fastine And Ionamin (Derivatives of Phentermine), Benzedrine, Didrex, Methylphenidate Or Ritalin, Phenmetrazine Or Preludin, Tenuate, Etc. PCP Unknown Type Drug Not too surprising, marijuana is the most common drug seized at 47% - or 455k incidents with it seized - of the data. This is followed by amphetamines/methamphetamines (including what we’d normally just call meth) at 20.7% and then heroin at 8.5%. Interestingly, cocaine and crack cocaine (which are always separate categories) both occur in 5.09% of drugs seized. Given the large disparity in sentences for these types of drugs, and that “crack wars” were a major purported cause of violent crime in the 1980s and 1990s, I expected crack cocaine to be much more common than normal cocaine. The remaining drug types are all less than 5% of drugs seized each and has some groupings of drug types (e.g. stimulants) rather than specific drug types (though some of these categories, such as LSD, are specific drugs). Table 7.4: The number and percent of drugs seized by police by type of drug, for all drugs seized in 2022. Drug Type First Year # of Drugs % of Drugs Marijuana 1991 557,044 44.09% Amphetamines/Methamphetamines 1991 301,759 23.88% Other Narcotics 1991 78,941 6.25% Cocaine (All Forms Except Crack) 1991 73,157 5.79% Heroin 1991 68,859 5.45% Crack Cocaine 1991 64,853 5.13% Other Drugs 1991 47,748 3.78% Unknown Type Drug 1991 26,787 2.12% Opium 1992 11,881 0.94% Other Hallucinogrens 1991 8,799 0.70% Hashish 1991 6,963 0.55% Other Depressants 1991 6,825 0.54% Other Stimulants 1991 3,996 0.32% Barbiturates 1991 1,582 0.13% PCP 1993 1,510 0.12% LSD 1991 1,461 0.12% Morphine 1992 1,271 0.10% Total 1,263,436 100% Figure 7.8: Annual percent of drug seizures by drug type, for the 1st drug reported, 1991-2022. 7.5.2 Amount of drugs For each drug type we know exactly how much was seized (at least we do other than for the 6.7% where the amount is “not reported”). Since different drug types are measured in different ways, this data also tells us what units the amount seized is in. So you’ll need to look at both the amount and the units to understand how much drugs were actually seized. The possible units are listed below: Dosage Unit/Items (Pills, Etc.) Fluid Ounce Gallon Gram Kilogram Liter Milliliter Number of Plants Ounce Pound Once you know the units you can look at the amount of drugs seized. The amount is specific up to the thousandths place though the integer and the numbers after the decimal point are actually in different columns in the data. For example, if police seized 1.257 grams of heroin, the 1 gram and the 0.257 grams would be in separate columns. As an example, we will look at Figure 7.9 which shows the number of grams seized for marijuana. This is only looking at the column with the amount in integers, so parts of a gram are excluded (but are available in the data). This also excludes any case where the marijuana seized was measured in a unit other than gram, such as number of plants, ounces, or pounds. Even though the data shows the number of grams as actual integers, not grouped at all, I do group the larger values together to make the graph simpler. So with those caveats, we can see what amounts of marijuana, measured in grams, are most frequently seized. Generally, the amount of marijuana seized is in small amounts with 22.5% being 1-2 grams (since we do not include the parts of a gram we can only say that it is 1 to 1.999 grams) and 18.6% being less than one gram. As the amount of drugs increase, the percent of seizures that involve this number of drugs decreases. It’s a bit curious that they include grams for values over 28 since 28.3495 grams is one ounce so it’d make sense to just start reporting in units of ounces instead of just increasingly large number of grams. Figure 7.9: For drugs seized that are measured in grams, this figure shows the distribution in the number of grams seized. Values over 10 grams are grouped together for easier interpretation of lower values of drugs seized. I would expect most property to be recovered on the arrestee’s body.↩︎ "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
